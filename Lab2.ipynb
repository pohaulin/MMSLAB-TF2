{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 實驗一：房價預測模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import必要套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 數據讀取並分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21613, 21)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"kc_house_data.csv\")\n",
    "# 顯示dataset的形狀，共21613比資料，每一比資料有21種不同資訊。\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  condition  grade  sqft_above  \\\n",
       "0      5650     1.0           0     0          3      7        1180   \n",
       "1      7242     2.0           0     0          3      7        2170   \n",
       "2     10000     1.0           0     0          3      6         770   \n",
       "3      5000     1.0           0     0          5      7        1050   \n",
       "4      8080     1.0           0     0          3      8        1680   \n",
       "\n",
       "   sqft_basement  yr_built  yr_renovated  zipcode      lat     long  \\\n",
       "0              0      1955             0    98178  47.5112 -122.257   \n",
       "1            400      1951          1991    98125  47.7210 -122.319   \n",
       "2              0      1933             0    98028  47.7379 -122.233   \n",
       "3            910      1965             0    98136  47.5208 -122.393   \n",
       "4              0      1987             0    98074  47.6168 -122.045   \n",
       "\n",
       "   sqft_living15  sqft_lot15  \n",
       "0           1340        5650  \n",
       "1           1690        7639  \n",
       "2           2720        8062  \n",
       "3           1360        5000  \n",
       "4           1800        7503  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 將顯示列數設定為25，不然會有部份資料無法顯示\n",
    "pd.options.display.max_columns = 25\n",
    "# head 會顯示前五行的數據\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各個數據的簡寫分別代表下面意思：\n",
    "- date：房屋出售日期。\n",
    "- price：房屋價格（目標）。\n",
    "- bedrooms：臥室數量。\n",
    "- bathrooms：浴室數量。\n",
    "- sqft_living：居住的坪數（平方英尺）。\n",
    "- sqft_lot：實際的坪數（平方英尺）。\n",
    "- floors：房屋總共樓層。\n",
    "- waterfront：海景房。\n",
    "- view：房屋是否看過。\n",
    "- condition：整體條件有多好。\n",
    "- grade：房屋的整體等級（根據King County評分系統）。\n",
    "- sqft_above：除了地下室外的坪數（平方英尺）。\n",
    "- sqft_basement：地下室的坪數（平方英尺）。\n",
    "- yr_built：房屋建造時間。\n",
    "- yr_renovated：何時重新裝修過（一些沒重新裝修過或是裝修紀錄沒被記錄到的數值都為0）。\n",
    "- zipcode：郵政編碼。\n",
    "- lat：緯度座標。\n",
    "- long：經度座標。\n",
    "- sqft_living15：2015年紀錄的居住坪數（可能是翻新的原因導致sqft_living15與sqft_living不同）。\n",
    "- sqft_lot15：2015年紀錄的實際坪數（可能是翻新的原因導致sqft_lot15與sqft_lot不同）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 檢查資料的型態\n",
    "\n",
    "資料型態總共有五種：object(string),booleab, integer, float and categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 int64\n",
       "date              object\n",
       "price            float64\n",
       "bedrooms           int64\n",
       "bathrooms        float64\n",
       "sqft_living        int64\n",
       "sqft_lot           int64\n",
       "floors           float64\n",
       "waterfront         int64\n",
       "view               int64\n",
       "condition          int64\n",
       "grade              int64\n",
       "sqft_above         int64\n",
       "sqft_basement      int64\n",
       "yr_built           int64\n",
       "yr_renovated       int64\n",
       "zipcode            int64\n",
       "lat              float64\n",
       "long             float64\n",
       "sqft_living15      int64\n",
       "sqft_lot15         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 數據前處理\n",
    "轉換資料型態：\n",
    "因為數據集裡的date數據是字串（string）格式，而模型的輸入只接受數值格式，所以可以透過以下程式碼將其轉為數值，並分成年、月及日三種數據。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "      <td>2014</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  waterfront  \\\n",
       "0  221900.0         3       1.00         1180      5650     1.0           0   \n",
       "1  538000.0         3       2.25         2570      7242     2.0           0   \n",
       "2  180000.0         2       1.00          770     10000     1.0           0   \n",
       "3  604000.0         4       3.00         1960      5000     1.0           0   \n",
       "4  510000.0         3       2.00         1680      8080     1.0           0   \n",
       "\n",
       "   view  condition  grade  sqft_above  sqft_basement  yr_built  yr_renovated  \\\n",
       "0     0          3      7        1180              0      1955             0   \n",
       "1     0          3      7        2170            400      1951          1991   \n",
       "2     0          3      6         770              0      1933             0   \n",
       "3     0          5      7        1050            910      1965             0   \n",
       "4     0          3      8        1680              0      1987             0   \n",
       "\n",
       "   zipcode      lat     long  sqft_living15  sqft_lot15  year  month  day  \n",
       "0    98178  47.5112 -122.257           1340        5650  2014     10   13  \n",
       "1    98125  47.7210 -122.319           1690        7639  2014     12    9  \n",
       "2    98028  47.7379 -122.233           2720        8062  2015      2   25  \n",
       "3    98136  47.5208 -122.393           1360        5000  2014     12    9  \n",
       "4    98074  47.6168 -122.045           1800        7503  2015      2   18  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 將date日期拆為年、月和日並轉成數值\n",
    "data['year'] = pd.to_numeric(data['date'].str.slice(0, 4))\n",
    "data['month'] = pd.to_numeric(data['date'].str.slice(4, 6))\n",
    "data['day'] = pd.to_numeric(data['date'].str.slice(6, 8))\n",
    "\n",
    "# 刪除沒有用的數據，inplace則是將更新後的資料存回原本的地方\n",
    "data.drop(['id'], axis=\"columns\", inplace=True)\n",
    "data.drop(['date'], axis=\"columns\", inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分割數據集（Dataset）：將數據集切割成三個部份，訓練數據（Training data）、驗證數據（Validation data）和測試數據（Testing data）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21613, 22)\n",
      "21613\n",
      "[  421  1129 20969 ... 13895 13866  2770]\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data_num = data.shape[0]\n",
    "print(data_num) #21613\n",
    "\n",
    "# 取得一筆與data數量相同的亂數索引，主要目的是用於打散資料\n",
    "indexes = np.random.permutation(data_num)\n",
    "print(indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  \\\n",
      "421     625000.0         3       1.75         1600      9135     1.0   \n",
      "1129    369000.0         5       1.50         2550      6300     1.0   \n",
      "20969   193000.0         2       1.75          910      2550     1.0   \n",
      "20165   865000.0         5       2.50         3190      8160     2.0   \n",
      "16204   234000.0         3       1.50         1140     10300     1.5   \n",
      "9554    408000.0         5       3.25         2820      6589     1.5   \n",
      "554     396000.0         3       1.50         1300      8280     1.0   \n",
      "20214   718000.0         5       2.75         2930      7663     2.0   \n",
      "5529    435000.0         3       2.50         1420      2581     3.0   \n",
      "6425    375000.0         5       1.75         2230      7560     1.0   \n",
      "8449    440000.0         3       1.75         2000      9900     1.0   \n",
      "14017   580000.0         2       1.00          860      4013     1.0   \n",
      "9264    273000.0         4       1.50         2180     22870     1.0   \n",
      "5316    321027.0         4       2.25         2820     16770     1.0   \n",
      "4453    470000.0         3       1.50         1500      5000     1.5   \n",
      "2721    223000.0         3       1.00         1030      9120     1.0   \n",
      "21021   420000.0         3       2.50         1509      1114     3.0   \n",
      "1705    229000.0         3       1.00         1370     56628     1.0   \n",
      "18473   539000.0         5       2.25         2590      7245     1.0   \n",
      "13514   515000.0         5       1.75         1880     48787     2.0   \n",
      "12190   615000.0         3       1.75         2220      7224     1.0   \n",
      "19894   472000.0         3       2.50         1860    415126     2.0   \n",
      "16171   960000.0         4       2.25         2410      4560     2.0   \n",
      "960     650000.0         4       2.00         2208      5000     3.0   \n",
      "14068   602500.0         4       1.75         2190     41000     2.0   \n",
      "12921   539000.0         3       1.50         1460      5040     1.0   \n",
      "21004   409316.0         3       2.50         1800      3168     2.0   \n",
      "17050   379400.0         4       1.75         2120      7680     1.0   \n",
      "19154   650000.0         4       1.75         2390     12000     1.0   \n",
      "1762    530000.0         4       2.25         2210      7665     2.0   \n",
      "...          ...       ...        ...          ...       ...     ...   \n",
      "3090    691000.0         2       2.00         1780      3810     1.5   \n",
      "3966    235500.0         5       2.50         2340     13713     1.0   \n",
      "12122   251000.0         3       1.00          840      4495     1.0   \n",
      "5397    200000.0         3       1.00         1010      8108     1.0   \n",
      "21554   388000.0         3       2.50         2198      6222     2.0   \n",
      "17051   190000.0         3       1.50          760     40039     1.0   \n",
      "3381    799950.0         3       3.00         2900     11769     2.0   \n",
      "19020   425000.0         3       2.50         1480      1386     3.0   \n",
      "17626   725000.0         3       1.75         1880     13300     1.0   \n",
      "18449   384950.0         3       1.00         1540      7740     1.0   \n",
      "13295   162000.0         4       1.00         1460     16638     1.0   \n",
      "10103   485000.0         4       3.25         1946     17786     2.0   \n",
      "17771   187000.0         2       1.75         1050      2926     1.0   \n",
      "15321   345000.0         3       1.00         1620     10610     1.0   \n",
      "15415  1650000.0         3       2.25         2750      6203     1.0   \n",
      "9939    893880.0         6       2.50         2820      8600     1.0   \n",
      "6057    650000.0         3       2.50         2790      6720     2.0   \n",
      "18989   370000.0         5       2.50         2250     10400     1.0   \n",
      "21489   352500.0         2       2.50          980      1010     3.0   \n",
      "529     662000.0         3       1.75         2500     36947     1.0   \n",
      "15561   510000.0         4       1.50         1320     14250     1.0   \n",
      "21113   320000.0         3       3.25         1480      1192     2.0   \n",
      "5770    375000.0         4       2.00         1620      4600     2.0   \n",
      "13855   775000.0         4       4.00         3180      7650     2.0   \n",
      "15227   603000.0         4       2.25         2110     11155     2.0   \n",
      "17172   275000.0         3       1.00          990      9798     1.0   \n",
      "19227   263000.0         4       2.75         1830      7315     1.0   \n",
      "20202   613500.0         3       3.25         1876      1531     3.0   \n",
      "12964   455500.0         3       1.75         2290     19000     1.0   \n",
      "11237   468000.0         4       2.50         3040     20682     2.0   \n",
      "\n",
      "       waterfront  view  condition  grade  sqft_above  sqft_basement  \\\n",
      "421             0     0          5      7        1600              0   \n",
      "1129            0     0          4      7        1560            990   \n",
      "20969           0     0          3      6         910              0   \n",
      "20165           0     0          3      9        3190              0   \n",
      "16204           0     0          4      6        1140              0   \n",
      "9554            0     0          3      7        2320            500   \n",
      "554             0     0          5      7        1300              0   \n",
      "20214           0     0          3      9        2930              0   \n",
      "5529            0     0          3      7        1420              0   \n",
      "6425            0     0          3      7        1230           1000   \n",
      "8449            0     2          4      8        1480            520   \n",
      "14017           0     0          3      7         860              0   \n",
      "9264            0     0          4      6        1280            900   \n",
      "5316            0     0          4      8        1920            900   \n",
      "4453            0     0          4      7        1140            360   \n",
      "2721            0     0          3      7        1030              0   \n",
      "21021           0     0          3      8        1509              0   \n",
      "1705            0     0          3      7        1370              0   \n",
      "18473           0     0          3      8        1510           1080   \n",
      "13514           0     0          3      6        1880              0   \n",
      "12190           0     2          3      8        2040            180   \n",
      "19894           0     0          3      7        1860              0   \n",
      "16171           0     2          5      9        1800            610   \n",
      "960             0     0          5      8        2208              0   \n",
      "14068           0     0          3      8        2190              0   \n",
      "12921           0     0          3      7        1100            360   \n",
      "21004           0     0          3      8        1800              0   \n",
      "17050           0     0          4      7        1060           1060   \n",
      "19154           0     0          3      8        1470            920   \n",
      "1762            0     0          4      8        2210              0   \n",
      "...           ...   ...        ...    ...         ...            ...   \n",
      "3090            0     0          3      7         980            800   \n",
      "3966            0     0          2      8        1670            670   \n",
      "12122           0     0          3      6         840              0   \n",
      "5397            0     0          3      7        1010              0   \n",
      "21554           0     2          3      8        2198              0   \n",
      "17051           0     0          3      6         760              0   \n",
      "3381            0     0          3     10        2900              0   \n",
      "19020           0     0          3      8        1480              0   \n",
      "17626           0     0          3      7        1380            500   \n",
      "18449           0     0          4      7        1540              0   \n",
      "13295           0     0          4      6        1460              0   \n",
      "10103           0     1          4      7        1946              0   \n",
      "17771           0     0          4      8        1050              0   \n",
      "15321           0     0          4      6        1620              0   \n",
      "15415           1     4          5      7        1620           1130   \n",
      "9939            0     0          5      8        1430           1390   \n",
      "6057            0     0          3      8        2790              0   \n",
      "18989           0     0          3      8        1280            970   \n",
      "21489           0     0          3      8         980              0   \n",
      "529             0     0          3      9        2500              0   \n",
      "15561           0     0          4      7        1320              0   \n",
      "21113           0     0          3      8        1180            300   \n",
      "5770            0     0          3      7        1620              0   \n",
      "13855           0     0          3      8        2530            650   \n",
      "15227           0     0          3      9        2110              0   \n",
      "17172           0     0          3      7         990              0   \n",
      "19227           0     0          5      7        1250            580   \n",
      "20202           0     0          3      9        1876              0   \n",
      "12964           0     0          4      9        1650            640   \n",
      "11237           0     0          3      7        3040              0   \n",
      "\n",
      "       yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
      "421        1955             0    98006  47.5724 -122.179           1580   \n",
      "1129       1959             0    98056  47.5014 -122.172           1380   \n",
      "20969      2004             0    98038  47.3494 -122.022           1310   \n",
      "20165      2014             0    98177  47.7246 -122.363           1650   \n",
      "16204      1967             0    98045  47.4452 -121.770           1250   \n",
      "9554       1906          2014    98118  47.5357 -122.273           1560   \n",
      "554        1956             0    98028  47.7403 -122.256           1570   \n",
      "20214      2013             0    98056  47.5308 -122.184           2750   \n",
      "5529       2004             0    98133  47.7027 -122.357           1420   \n",
      "6425       1959             0    98178  47.5055 -122.260           1380   \n",
      "8449       1957             0    98166  47.4436 -122.339           2310   \n",
      "14017      1925             0    98103  47.6652 -122.338           1490   \n",
      "9264       1954          1975    98042  47.3187 -122.081           2420   \n",
      "5316       1966             0    98023  47.3186 -122.364           2320   \n",
      "4453       1927             0    98117  47.6790 -122.373           1500   \n",
      "2721       1961             0    98032  47.3607 -122.291           1470   \n",
      "21021      2014             0    98133  47.7049 -122.340           1509   \n",
      "1705       1942             0    98003  47.3058 -122.306           1768   \n",
      "18473      1973             0    98008  47.6398 -122.111           1930   \n",
      "13514      1922             0    98059  47.5094 -122.165           1690   \n",
      "12190      1975             0    98177  47.7740 -122.384           2540   \n",
      "19894      2006             0    98038  47.3974 -122.005           2070   \n",
      "16171      1929             0    98117  47.6796 -122.402           2150   \n",
      "960        1917             0    98116  47.5711 -122.383           1760   \n",
      "14068      1980             0    98075  47.5755 -122.033           2590   \n",
      "12921      1971             0    98133  47.7112 -122.357           2330   \n",
      "21004      2014             0    98065  47.5342 -121.841           1800   \n",
      "17050      1950             0    98177  47.7172 -122.361           1530   \n",
      "19154      1979             0    98052  47.7061 -122.163           2110   \n",
      "1762       1968             0    98006  47.5424 -122.168           1960   \n",
      "...         ...           ...      ...      ...      ...            ...   \n",
      "3090       1922             0    98119  47.6474 -122.362           1690   \n",
      "3966       1967             0    98003  47.3307 -122.324           2080   \n",
      "12122      1921             0    98146  47.4960 -122.349           1260   \n",
      "5397       1955             0    98155  47.7350 -122.309           1110   \n",
      "21554      2010             0    98198  47.3906 -122.304           2198   \n",
      "17051      1906             0    98065  47.5295 -121.829            980   \n",
      "3381       1997             0    98052  47.6993 -122.118           2900   \n",
      "19020      2005             0    98029  47.5468 -121.998           1470   \n",
      "17626      1967             0    98004  47.6340 -122.204           3550   \n",
      "18449      1909             0    98126  47.5220 -122.375           1220   \n",
      "13295      1975             0    98010  47.3431 -122.048           1460   \n",
      "10103      1990             0    98070  47.3590 -122.452           1460   \n",
      "17771      1974             0    98198  47.3811 -122.317           1150   \n",
      "15321      1958             0    98059  47.4740 -122.125           1680   \n",
      "15415      1959             0    98074  47.6163 -122.068           2570   \n",
      "9939       1967             0    98006  47.5650 -122.144           2070   \n",
      "6057       2002             0    98075  47.5958 -122.038           2620   \n",
      "18989      1973             0    98058  47.4501 -122.139           2140   \n",
      "21489      2008             0    98117  47.6844 -122.387            980   \n",
      "529        1984             0    98052  47.6917 -122.084           2590   \n",
      "15561      1954             0    98033  47.7016 -122.199           1720   \n",
      "21113      2013             0    98106  47.5556 -122.363           1330   \n",
      "5770       1909             0    98108  47.5533 -122.307           1620   \n",
      "13855      1920             0    98115  47.6887 -122.319           2000   \n",
      "15227      1975             0    98074  47.6386 -122.058           2660   \n",
      "17172      1968             0    98072  47.7670 -122.164           1210   \n",
      "19227      1979             0    98023  47.2989 -122.380           1730   \n",
      "20202      2009             0    98115  47.6864 -122.265           1876   \n",
      "12964      1973             0    98042  47.3837 -122.158           2900   \n",
      "11237      2000             0    98019  47.7245 -121.959           2670   \n",
      "\n",
      "       sqft_lot15  year  month  day  \n",
      "421          9800  2014      7   30  \n",
      "1129         6300  2015      2   13  \n",
      "20969        2550  2015      3   26  \n",
      "20165        8160  2014      7   18  \n",
      "16204        9975  2014     10    1  \n",
      "9554         5647  2014      6   30  \n",
      "554          8692  2014     11   17  \n",
      "20214       10335  2014     10    3  \n",
      "5529         2509  2014      6   11  \n",
      "6425         6570  2015      3   24  \n",
      "8449        10200  2014      6   24  \n",
      "14017        4013  2015      3   13  \n",
      "9264        22614  2014      5   13  \n",
      "5316        13850  2015      2    3  \n",
      "4453         5000  2014      6   28  \n",
      "2721        10220  2014      6    3  \n",
      "21021        2431  2015      4   15  \n",
      "1705         8702  2014      8   11  \n",
      "18473        7245  2015      2   25  \n",
      "13514        8401  2014     12   23  \n",
      "12190        9990  2014     12    1  \n",
      "19894       54014  2015      5    7  \n",
      "16171        5100  2014      5   28  \n",
      "960          5000  2014     11   12  \n",
      "14068       35370  2015      4   13  \n",
      "12921        7560  2015      1   20  \n",
      "21004        3393  2014      5   20  \n",
      "17050        7680  2014     11   10  \n",
      "19154       12000  2014      7   25  \n",
      "1762         7903  2014      6    5  \n",
      "...           ...   ...    ...  ...  \n",
      "3090         3810  2014      9   22  \n",
      "3966        11000  2014      7    8  \n",
      "12122        7434  2015      4   21  \n",
      "5397         8108  2014      9    8  \n",
      "21554        7621  2014      8    8  \n",
      "17051        6000  2014     12   30  \n",
      "3381         9611  2014     11    7  \n",
      "19020        1593  2015      3   10  \n",
      "17626       10883  2014      7   31  \n",
      "18449        7740  2014      9    2  \n",
      "13295       16638  2015      1    7  \n",
      "10103       16661  2015      4   14  \n",
      "17771        3802  2014      8   15  \n",
      "15321       10795  2014     11   25  \n",
      "15415        7009  2014     11   22  \n",
      "9939         8900  2015      4   13  \n",
      "6057         6720  2014      8   26  \n",
      "18989        9592  2014     11   14  \n",
      "21489        1023  2015      1   20  \n",
      "529         28837  2014     10    8  \n",
      "15561       14250  2014      6    6  \n",
      "21113        1094  2014     12   11  \n",
      "5770         4500  2015      4   27  \n",
      "13855        4080  2014     10   17  \n",
      "15227       11900  2015      4   17  \n",
      "17172        9870  2014      6   24  \n",
      "19227        7208  2014     10    2  \n",
      "20202        1533  2015      5    1  \n",
      "12964       13034  2015      2    3  \n",
      "11237        9742  2015      3   19  \n",
      "\n",
      "[12967 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# 並將亂數索引值分為Train、validation和test分為，這裡的劃分比例為6:2:2\n",
    "train_indexes = indexes[:int(data_num *0.6)]\n",
    "val_indexes = indexes[int(data_num *0.6):int(data_num *0.8)]\n",
    "test_indexes = indexes[int(data_num *0.8):]\n",
    "# 透過索引值從data取出訓練資料、驗證資料和測試資料\n",
    "train_data = data.loc[train_indexes]\n",
    "val_data = data.loc[val_indexes]\n",
    "test_data = data.loc[test_indexes]\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization 正規化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用標準分數(Standard Score, 又稱z-score)將數據正規化，經過z-score正規化後數據的都會聚集在0附近， 標準差為1。 \n",
    "\n",
    "(x - 平均值) / 標準差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          price  bedrooms  bathrooms  sqft_living  sqft_lot    floors  \\\n",
      "421    0.230538 -0.393634  -0.472403    -0.524759 -0.142556 -0.916012   \n",
      "1129  -0.470186  1.737881  -0.797400     0.514461 -0.209029 -0.916012   \n",
      "20969 -0.951933 -1.459391  -0.472403    -1.279561 -0.296957 -0.916012   \n",
      "20165  0.887466  1.737881   0.502590     1.214567 -0.165417  0.933016   \n",
      "16204 -0.839708 -0.393634  -0.797400    -1.027961 -0.115239  0.008502   \n",
      "9554  -0.363435  1.737881   1.477583     0.809818 -0.202253  0.008502   \n",
      "554   -0.396282 -0.393634  -0.797400    -0.852934 -0.162603 -0.916012   \n",
      "20214  0.485097  1.737881   0.827588     0.930149 -0.177070  0.933016   \n",
      "5529  -0.289531 -0.393634   0.502590    -0.721664 -0.296230  2.782043   \n",
      "6425  -0.453763  1.737881  -0.472403     0.164408 -0.179485 -0.916012   \n",
      "8449  -0.275845 -0.393634  -0.472403    -0.087193 -0.124618 -0.916012   \n",
      "14017  0.107364 -1.459391  -1.447396    -1.334257 -0.262653 -0.916012   \n",
      "9264  -0.732957  0.672124  -0.797400     0.109712  0.179495 -0.916012   \n",
      "5316  -0.601498  0.672124   0.177593     0.809818  0.036466 -0.916012   \n",
      "4453  -0.193729 -0.393634  -0.797400    -0.634151 -0.239511  0.008502   \n",
      "2721  -0.869817 -0.393634  -1.447396    -1.148291 -0.142907 -0.916012   \n",
      "21021 -0.330589 -0.393634   0.502590    -0.624306 -0.330628  2.782043   \n",
      "1705  -0.853394 -0.393634  -1.447396    -0.776360  0.971034 -0.916012   \n",
      "18473 -0.004862  1.737881   0.177593     0.558218 -0.186871 -0.916012   \n",
      "13514 -0.070555  1.737881  -0.472403    -0.218463  0.787183  0.933016   \n",
      "12190  0.203166 -0.393634  -0.472403     0.153469 -0.187364 -0.916012   \n",
      "19894 -0.188254 -0.393634   0.502590    -0.240341  9.376900  0.933016   \n",
      "16171  1.147500  0.672124   0.177593     0.361313 -0.249828  0.933016   \n",
      "960    0.298968  0.672124  -0.147405     0.140342 -0.239511  2.782043   \n",
      "14068  0.168951  0.672124  -0.472403     0.120651  0.604598  0.933016   \n",
      "12921 -0.004862 -0.393634  -0.797400    -0.677907 -0.238573 -0.916012   \n",
      "21004 -0.359833 -0.393634   0.502590    -0.305976 -0.282467  0.933016   \n",
      "17050 -0.441719  0.672124  -0.472403     0.044077 -0.176672 -0.916012   \n",
      "19154  0.298968  0.672124  -0.472403     0.339434 -0.075379 -0.916012   \n",
      "1762  -0.029497  0.672124   0.177593     0.142530 -0.177023  0.933016   \n",
      "...         ...       ...        ...          ...       ...       ...   \n",
      "3090   0.411193 -1.459391  -0.147405    -0.327854 -0.267413  0.008502   \n",
      "3966  -0.835602  1.737881   0.502590     0.284739 -0.035213 -0.916012   \n",
      "12122 -0.793176 -0.393634  -1.447396    -1.356135 -0.251352 -0.916012   \n",
      "5397  -0.932773 -0.393634  -1.447396    -1.170170 -0.166636 -0.916012   \n",
      "21554 -0.418179 -0.393634   0.502590     0.129403 -0.210858  0.933016   \n",
      "17051 -0.960145 -0.393634  -0.797400    -1.443649  0.582065 -0.916012   \n",
      "3381   0.709411 -0.393634   1.152586     0.897332 -0.080795  0.933016   \n",
      "19020 -0.316903 -0.393634   0.502590    -0.656029 -0.324250  2.782043   \n",
      "17626  0.504258 -0.393634  -0.472403    -0.218463 -0.044897 -0.916012   \n",
      "18449 -0.426528 -0.393634  -1.447396    -0.590394 -0.175265 -0.916012   \n",
      "13295 -1.036787  0.672124  -1.447396    -0.677907  0.033371 -0.916012   \n",
      "10103 -0.152671  0.672124   1.477583    -0.146264  0.060288  0.933016   \n",
      "17771 -0.968357 -1.459391  -0.472403    -1.126413 -0.288141 -0.916012   \n",
      "15321 -0.535879 -0.393634  -1.447396    -0.502881 -0.107971 -0.916012   \n",
      "15415  3.036169 -0.393634   0.177593     0.733244 -0.211304 -0.916012   \n",
      "9939   0.966516  2.803638   0.502590     0.809818 -0.155100 -0.916012   \n",
      "6057   0.298968 -0.393634   0.502590     0.777001 -0.199181  0.933016   \n",
      "18989 -0.467449  1.737881   0.502590     0.186286 -0.112895 -0.916012   \n",
      "21489 -0.515350 -1.459391   0.502590    -1.202987 -0.333066  2.782043   \n",
      "529    0.331814 -0.393634  -0.472403     0.459765  0.509565 -0.916012   \n",
      "15561 -0.084241  0.672124  -0.797400    -0.831056 -0.022622 -0.916012   \n",
      "21113 -0.604309 -0.393634   1.477583    -0.656029 -0.328799  0.933016   \n",
      "5770  -0.453763  0.672124  -0.147405    -0.502881 -0.248890  0.933016   \n",
      "13855  0.641118  0.672124   2.452576     1.203628 -0.177375  0.933016   \n",
      "15227  0.170319  0.672124   0.177593     0.033138 -0.095192  0.933016   \n",
      "17172 -0.727483 -0.393634  -1.447396    -1.192048 -0.127010 -0.916012   \n",
      "19227 -0.760329  0.672124   0.827588    -0.273159 -0.185230 -0.916012   \n",
      "20202  0.199060 -0.393634   1.477583    -0.222838 -0.320850  2.782043   \n",
      "12964 -0.233418 -0.393634  -0.472403     0.230043  0.088754 -0.916012   \n",
      "11237 -0.199203  0.672124   0.502590     1.050480  0.128192  0.933016   \n",
      "\n",
      "       waterfront      view  condition     grade  sqft_above  sqft_basement  \\\n",
      "421     -0.089695 -0.309230   2.454009 -0.559849   -0.228306      -0.658383   \n",
      "1129    -0.089695 -0.309230   0.914577 -0.559849   -0.276773       1.579282   \n",
      "20969   -0.089695 -0.309230  -0.624854 -1.410445   -1.064362      -0.658383   \n",
      "20165   -0.089695 -0.309230  -0.624854  1.141344    1.698258      -0.658383   \n",
      "16204   -0.089695 -0.309230   0.914577 -1.410445   -0.785676      -0.658383   \n",
      "9554    -0.089695 -0.309230  -0.624854 -0.559849    0.644100       0.471751   \n",
      "554     -0.089695 -0.309230   2.454009 -0.559849   -0.591808      -0.658383   \n",
      "20214   -0.089695 -0.309230  -0.624854  1.141344    1.383222      -0.658383   \n",
      "5529    -0.089695 -0.309230  -0.624854 -0.559849   -0.446407      -0.658383   \n",
      "6425    -0.089695 -0.309230  -0.624854 -0.559849   -0.676626       1.601885   \n",
      "8449    -0.089695  2.275534   0.914577  0.290748   -0.373707       0.516956   \n",
      "14017   -0.089695 -0.309230  -0.624854 -0.559849   -1.124945      -0.658383   \n",
      "9264    -0.089695 -0.309230   0.914577 -1.410445   -0.616042       1.375858   \n",
      "5316    -0.089695 -0.309230   0.914577  0.290748    0.159430       1.375858   \n",
      "4453    -0.089695 -0.309230   0.914577 -0.559849   -0.785676       0.155313   \n",
      "2721    -0.089695 -0.309230  -0.624854 -0.559849   -0.918961      -0.658383   \n",
      "21021   -0.089695 -0.309230  -0.624854  0.290748   -0.338568      -0.658383   \n",
      "1705    -0.089695 -0.309230  -0.624854 -0.559849   -0.506991      -0.658383   \n",
      "18473   -0.089695 -0.309230  -0.624854  0.290748   -0.337357       1.782707   \n",
      "13514   -0.089695 -0.309230  -0.624854 -1.410445    0.110963      -0.658383   \n",
      "12190   -0.089695  2.275534  -0.624854  0.290748    0.304831      -0.251535   \n",
      "19894   -0.089695 -0.309230  -0.624854 -0.559849    0.086730      -0.658383   \n",
      "16171   -0.089695  2.275534   2.454009  1.141344    0.014029       0.720380   \n",
      "960     -0.089695 -0.309230   2.454009  0.290748    0.508393      -0.658383   \n",
      "14068   -0.089695 -0.309230  -0.624854  0.290748    0.486583      -0.658383   \n",
      "12921   -0.089695 -0.309230  -0.624854 -0.559849   -0.834143       0.155313   \n",
      "21004   -0.089695 -0.309230  -0.624854  0.290748    0.014029      -0.658383   \n",
      "17050   -0.089695 -0.309230   0.914577 -0.559849   -0.882610       1.737501   \n",
      "19154   -0.089695 -0.309230  -0.624854  0.290748   -0.385824       1.421064   \n",
      "1762    -0.089695 -0.309230   0.914577  0.290748    0.510816      -0.658383   \n",
      "...           ...       ...        ...       ...         ...            ...   \n",
      "3090    -0.089695 -0.309230  -0.624854 -0.559849   -0.979544       1.149831   \n",
      "3966    -0.089695 -0.309230  -2.164286  0.290748   -0.143489       0.855997   \n",
      "12122   -0.089695 -0.309230  -0.624854 -1.410445   -1.149179      -0.658383   \n",
      "5397    -0.089695 -0.309230  -0.624854 -0.559849   -0.943194      -0.658383   \n",
      "21554   -0.089695  2.275534  -0.624854  0.290748    0.496276      -0.658383   \n",
      "17051   -0.089695 -0.309230  -0.624854 -1.410445   -1.246113      -0.658383   \n",
      "3381    -0.089695 -0.309230  -0.624854  1.991940    1.346872      -0.658383   \n",
      "19020   -0.089695 -0.309230  -0.624854  0.290748   -0.373707      -0.658383   \n",
      "17626   -0.089695 -0.309230  -0.624854 -0.559849   -0.494874       0.471751   \n",
      "18449   -0.089695 -0.309230   0.914577 -0.559849   -0.301006      -0.658383   \n",
      "13295   -0.089695 -0.309230   0.914577 -1.410445   -0.397940      -0.658383   \n",
      "10103   -0.089695  0.983152   0.914577 -0.559849    0.190934      -0.658383   \n",
      "17771   -0.089695 -0.309230   0.914577  0.290748   -0.894727      -0.658383   \n",
      "15321   -0.089695 -0.309230   0.914577 -1.410445   -0.204072      -0.658383   \n",
      "15415   11.148214  4.860299   2.454009 -0.559849   -0.204072       1.895720   \n",
      "9939    -0.089695 -0.309230   2.454009  0.290748   -0.434291       2.483390   \n",
      "6057    -0.089695 -0.309230  -0.624854  0.290748    1.213588      -0.658383   \n",
      "18989   -0.089695 -0.309230  -0.624854  0.290748   -0.616042       1.534077   \n",
      "21489   -0.089695 -0.309230  -0.624854  0.290748   -0.979544      -0.658383   \n",
      "529     -0.089695 -0.309230  -0.624854  1.141344    0.862202      -0.658383   \n",
      "15561   -0.089695 -0.309230   0.914577 -0.559849   -0.567575      -0.658383   \n",
      "21113   -0.089695 -0.309230  -0.624854  0.290748   -0.737209       0.019697   \n",
      "5770    -0.089695 -0.309230  -0.624854 -0.559849   -0.204072      -0.658383   \n",
      "13855   -0.089695 -0.309230  -0.624854  0.290748    0.898552       0.810791   \n",
      "15227   -0.089695 -0.309230  -0.624854  1.141344    0.389649      -0.658383   \n",
      "17172   -0.089695 -0.309230  -0.624854 -0.559849   -0.967428      -0.658383   \n",
      "19227   -0.089695 -0.309230   2.454009 -0.559849   -0.652392       0.652572   \n",
      "20202   -0.089695 -0.309230  -0.624854  1.141344    0.106117      -0.658383   \n",
      "12964   -0.089695 -0.309230   0.914577  1.141344   -0.167722       0.788189   \n",
      "11237   -0.089695 -0.309230  -0.624854 -0.559849    1.516506      -0.658383   \n",
      "\n",
      "       yr_built  yr_renovated   zipcode       lat      long  sqft_living15  \\\n",
      "421   -0.546283     -0.213818 -1.344887  0.085895  0.248945      -0.589365   \n",
      "1129  -0.409943     -0.213818 -0.411319 -0.427079  0.298803      -0.879842   \n",
      "20969  1.123884     -0.213818 -0.747403 -1.525276  1.367198      -0.981509   \n",
      "20165  1.464734     -0.213818  1.847916  1.185538 -1.061620      -0.487698   \n",
      "16204 -0.137263     -0.213818 -0.616704 -0.833123  3.162102      -1.068652   \n",
      "9554  -2.216450      4.721211  0.746306 -0.179262 -0.420583      -0.618413   \n",
      "554   -0.512198     -0.213818 -0.934117  1.298970 -0.299498      -0.603889   \n",
      "20214  1.430649     -0.213818 -0.411319 -0.214664  0.213331       1.109925   \n",
      "5529   1.123884     -0.213818  1.026376  1.027311 -1.018884      -0.821746   \n",
      "6425  -0.409943     -0.213818  1.866588 -0.397456 -0.327989      -0.879842   \n",
      "8449  -0.478113     -0.213818  1.642531 -0.844683 -0.890677       0.470876   \n",
      "14017 -1.568835     -0.213818  0.466236  0.756374 -0.883554      -0.720079   \n",
      "9264  -0.580368      4.625647 -0.672718 -1.747083  0.946963       0.630638   \n",
      "5316  -0.171348     -0.213818 -1.027474 -1.747806 -1.068743       0.485399   \n",
      "4453  -1.500664     -0.213818  0.727635  0.856078 -1.132847      -0.705556   \n",
      "2721  -0.341773     -0.213818 -0.859431 -1.443634 -0.548791      -0.749127   \n",
      "21021  1.464734     -0.213818  1.026376  1.043205 -0.897800      -0.692484   \n",
      "1705  -0.989389     -0.213818 -1.400901 -1.840286 -0.655630      -0.316317   \n",
      "18473  0.067247     -0.213818 -1.307544  0.572859  0.733284      -0.081030   \n",
      "13514 -1.671090     -0.213818 -0.355305 -0.369279  0.348661      -0.429603   \n",
      "12190  0.135418     -0.213818  1.847916  1.542452 -1.211196       0.804924   \n",
      "19894  1.192054     -0.213818 -0.747403 -1.178477  1.488283       0.122303   \n",
      "16171 -1.432494     -0.213818  0.727635  0.860413 -1.339403       0.238494   \n",
      "960   -1.841515     -0.213818  0.708963  0.076503 -1.204073      -0.327936   \n",
      "14068  0.305843     -0.213818 -0.056563  0.108293  1.288849       0.877543   \n",
      "12921 -0.000923     -0.213818  1.026376  1.088723 -1.018884       0.499923   \n",
      "21004  1.464734     -0.213818 -0.243276 -0.190099  2.656395      -0.269840   \n",
      "17050 -0.716708     -0.213818  1.847916  1.132073 -1.047375      -0.661984   \n",
      "19154  0.271758     -0.213818 -0.486004  1.051875  0.362907       0.180399   \n",
      "1762  -0.103178     -0.213818 -1.344887 -0.130854  0.327294      -0.037459   \n",
      "...         ...           ...       ...       ...       ...            ...   \n",
      "3090  -1.671090     -0.213818  0.764977  0.627769 -1.054498      -0.429603   \n",
      "3966  -0.137263     -0.213818 -1.400901 -1.660384 -0.783837       0.136827   \n",
      "12122 -1.705175     -0.213818  1.269104 -0.466094 -0.961903      -1.054128   \n",
      "5397  -0.546283     -0.213818  1.437146  1.260677 -0.676998      -1.271985   \n",
      "21554  1.328394     -0.213818  2.240015 -1.227607 -0.641385       0.308209   \n",
      "17051 -2.216450     -0.213818 -0.243276 -0.224057  2.741867      -1.460795   \n",
      "3381   0.885289     -0.213818 -0.486004  1.002746  0.683425       1.327782   \n",
      "19020  1.157969     -0.213818 -0.915445 -0.099065  1.538141      -0.749127   \n",
      "17626 -0.137263     -0.213818 -1.382230  0.530954  0.070879       2.271832   \n",
      "18449 -2.114195     -0.213818  0.895677 -0.278244 -1.147092      -1.112223   \n",
      "13295  0.135418     -0.213818 -1.270201 -1.570794  1.182010      -0.763651   \n",
      "10103  0.646693     -0.213818 -0.149919 -1.455917 -1.695535      -0.763651   \n",
      "17771  0.101333     -0.213818  2.240015 -1.296244 -0.733979      -1.213890   \n",
      "15321 -0.444028     -0.213818 -0.355305 -0.625043  0.633567      -0.444126   \n",
      "15415 -0.409943     -0.213818 -0.075234  0.403072  1.039557       0.848495   \n",
      "9939  -0.137263     -0.213818 -1.344887  0.032430  0.498237       0.122303   \n",
      "6057   1.055714     -0.213818 -0.056563  0.254960  1.253236       0.921115   \n",
      "18989  0.067247     -0.213818 -0.373976 -0.797720  0.533850       0.223970   \n",
      "21489  1.260224     -0.213818  0.727635  0.895093 -1.232563      -1.460795   \n",
      "529    0.442183     -0.213818 -0.486004  0.947836  0.925595       0.877543   \n",
      "15561 -0.580368     -0.213818 -0.840760  1.019363  0.106492      -0.386031   \n",
      "21113  1.430649     -0.213818  0.522250 -0.035485 -1.061620      -0.952461   \n",
      "5770  -2.114195     -0.213818  0.559592 -0.052102 -0.662753      -0.531270   \n",
      "13855 -1.739260     -0.213818  0.690292  0.926161 -0.748224       0.020636   \n",
      "15227  0.135418     -0.213818 -0.075234  0.564189  1.110783       0.979210   \n",
      "17172 -0.103178     -0.213818 -0.112577  1.491877  0.355784      -1.126747   \n",
      "19227  0.271758     -0.213818 -1.027474 -1.890138 -1.182705      -0.371507   \n",
      "20202  1.294309     -0.213818  0.690292  0.909543 -0.363602      -0.159459   \n",
      "12964  0.067247     -0.213818 -0.672718 -1.277459  0.398520       1.327782   \n",
      "11237  0.987544     -0.213818 -1.102159  1.184815  1.815924       0.993734   \n",
      "\n",
      "       sqft_lot15      year     month       day  \n",
      "421     -0.108132 -0.690738  0.137375  1.650748  \n",
      "1129    -0.233494  1.447643 -1.467063 -0.312712  \n",
      "20969   -0.367810  1.447643 -1.146176  1.188757  \n",
      "20165   -0.166873 -0.690738  0.137375  0.264776  \n",
      "16204   -0.101864 -0.690738  1.100038 -1.698684  \n",
      "9554    -0.256883 -0.690738 -0.183513  1.650748  \n",
      "554     -0.147818 -0.690738  1.420925  0.149279  \n",
      "20214   -0.088970 -0.690738  1.100038 -1.467688  \n",
      "5529    -0.369279 -0.690738 -0.183513 -0.543707  \n",
      "6425    -0.223823  1.447643 -1.146176  0.957762  \n",
      "8449    -0.093805 -0.690738 -0.183513  0.957762  \n",
      "14017   -0.315409  1.447643 -1.146176 -0.312712  \n",
      "9264     0.350835 -0.690738 -0.504400 -0.312712  \n",
      "5316     0.036929  1.447643 -1.467063 -1.467688  \n",
      "4453    -0.280057 -0.690738 -0.183513  1.419752  \n",
      "2721    -0.093089 -0.690738 -0.183513 -1.467688  \n",
      "21021   -0.372073  1.447643 -0.825288 -0.081717  \n",
      "1705    -0.147460 -0.690738  0.458262 -0.543707  \n",
      "18473   -0.199646  1.447643 -1.467063  1.073260  \n",
      "13514   -0.158241 -0.690738  1.741813  0.842264  \n",
      "12190   -0.101327 -0.690738  1.741813 -1.698684  \n",
      "19894    1.475510  1.447643 -0.504400 -1.005698  \n",
      "16171   -0.276475 -0.690738 -0.504400  1.419752  \n",
      "960     -0.280057 -0.690738  1.420925 -0.428210  \n",
      "14068    0.807726  1.447643 -0.825288 -0.312712  \n",
      "12921   -0.188364  1.447643 -1.787951  0.495771  \n",
      "21004   -0.337616 -0.690738 -0.504400  0.495771  \n",
      "17050   -0.184066 -0.690738  1.420925 -0.659205  \n",
      "19154   -0.029333 -0.690738  0.137375  1.073260  \n",
      "1762    -0.176078 -0.690738 -0.183513 -1.236693  \n",
      "...           ...       ...       ...       ...  \n",
      "3090    -0.322680 -0.690738  0.779150  0.726767  \n",
      "3966    -0.065151 -0.690738  0.137375 -0.890200  \n",
      "12122   -0.192877  1.447643 -0.825288  0.611269  \n",
      "5397    -0.168736 -0.690738  0.779150 -0.890200  \n",
      "21554   -0.186179 -0.690738  0.458262 -0.890200  \n",
      "17051   -0.244239 -0.690738  1.741813  1.650748  \n",
      "3381    -0.114902 -0.690738  1.420925 -1.005698  \n",
      "19020   -0.402088  1.447643 -1.146176 -0.659205  \n",
      "17626   -0.069342 -0.690738  0.137375  1.766245  \n",
      "18449   -0.181917 -0.690738  0.779150 -1.583186  \n",
      "13295    0.136789  1.447643 -1.787951 -1.005698  \n",
      "10103    0.137613  1.447643 -0.825288 -0.197214  \n",
      "17771   -0.322967 -0.690738  0.458262 -0.081717  \n",
      "15321   -0.072494 -0.690738  1.420925  1.073260  \n",
      "15415   -0.208099 -0.690738  1.420925  0.726767  \n",
      "9939    -0.140368  1.447643 -0.825288 -0.312712  \n",
      "6057    -0.218451 -0.690738  0.458262  1.188757  \n",
      "18989   -0.115582 -0.690738  1.420925 -0.197214  \n",
      "21489   -0.422504  1.447643 -1.787951  0.495771  \n",
      "529      0.573729 -0.690738  1.100038 -0.890200  \n",
      "15561    0.051256 -0.690738 -0.183513 -1.121195  \n",
      "21113   -0.419961 -0.690738  1.741813 -0.543707  \n",
      "5770    -0.297966  1.447643 -0.825288  1.304255  \n",
      "13855   -0.313009 -0.690738  1.100038  0.149279  \n",
      "15227   -0.032915  1.447643 -0.825288  0.149279  \n",
      "17172   -0.105625 -0.690738 -0.183513  0.957762  \n",
      "19227   -0.200972 -0.690738  1.100038 -1.583186  \n",
      "20202   -0.404237  1.447643 -0.504400 -1.698684  \n",
      "12964    0.007702  1.447643 -1.467063 -1.467688  \n",
      "11237   -0.110210  1.447643 -1.146176  0.380274  \n",
      "\n",
      "[12967 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "train_validation_data = pd.concat([train_data, val_data])\n",
    "mean = train_validation_data.mean()\n",
    "std = train_validation_data.std()\n",
    "\n",
    "train_data = (train_data - mean) / std\n",
    "val_data = (val_data - mean) / std\n",
    "\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立Numpy array格式的訓練數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(train_data.drop(columns=['price']))\n",
    "y_train = np.array(train_data['price'])\n",
    "x_val = np.array(val_data.drop('price', axis='columns'))\n",
    "y_val = np.array(val_data['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整理過後的資料共12967筆，且一筆資料有21種資訊(所以網路輸入必須為21)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12967, 21)\n",
      "(4323, 21)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立並訓練網路模型\n",
    "\n",
    "這裡建構三層全連接層的網路架構，並且使用ReLU作為隱藏層的激活函數，而由於需得到線性輸出，故輸出層不使用任何激活函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-716bfbe59e2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 建立一個Sequential型態的model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# 第1層全連接層設為64個unit，將輸入形狀設定為(21, )，而實際上我們輸入的數據形狀為(batch_size, 21)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 第2層全連接層設為64個unit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "# 建立一個Sequential型態的model\n",
    "model = keras.Sequential(name='model-1')\n",
    "# 第1層全連接層設為64個unit，將輸入形狀設定為(21, )，而實際上我們輸入的數據形狀為(batch_size, 21)\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(21,)))\n",
    "# 第2層全連接層設為64個unit\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "# 最後一層全連接層設為1個unit\n",
    "model.add(layers.Dense(1))\n",
    "# 顯示網路模型架構\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "設定訓練使用的優化器、損失函數和指標函數："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a79f91f5b297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.compile(keras.optimizers.Adam(0.001),\n\u001b[0m\u001b[1;32m      2\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeanSquaredError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m               metrics=[keras.metrics.MAE])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(keras.optimizers.Adam(0.001),\n",
    "              loss=keras.losses.MeanSquaredError(),\n",
    "              metrics=[keras.metrics.MAE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "創建模型儲存目錄："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ac0d4037370f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'lab2-logs-v1/models/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "model_dir = 'lab2-logs-v1/models/'\n",
    "os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "設定回調函數："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-27e804ddcad3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TensorBoard回調函數會幫忙紀錄訓練資訊，並存成TensorBoard的紀錄檔\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlog_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lab2-logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel_cbk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ModelCheckpoint回調函數幫忙儲存網路模型，可以設定只儲存最好的模型，「monitor」表示被監測的數據，「mode」min則代表監測數據越小越好。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m model_mckp = keras.callbacks.ModelCheckpoint(model_dir + '/Best-model-1.h5', \n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# TensorBoard回調函數會幫忙紀錄訓練資訊，並存成TensorBoard的紀錄檔\n",
    "log_dir = os.path.join('lab2-logs', 'model-1')\n",
    "model_cbk = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "# ModelCheckpoint回調函數幫忙儲存網路模型，可以設定只儲存最好的模型，「monitor」表示被監測的數據，「mode」min則代表監測數據越小越好。\n",
    "model_mckp = keras.callbacks.ModelCheckpoint(model_dir + '/Best-model-1.h5', \n",
    "                                        monitor='val_mean_absolute_error', \n",
    "                                        save_best_only=True, \n",
    "                                        mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練網路模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d195f2b1d769>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(x_train, y_train,  # 傳入訓練數據\n\u001b[0m\u001b[1;32m      2\u001b[0m                \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# 批次大小設為64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# 整個dataset訓練300遍\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# 驗證數據\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                callbacks=[model_cbk, model_mckp])  # Tensorboard回調函數紀錄訓練過程，ModelCheckpoint回調函數儲存最好的模型\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,  # 傳入訓練數據\n",
    "               batch_size=64,  # 批次大小設為64\n",
    "               epochs=300,  # 整個dataset訓練300遍\n",
    "               validation_data=(x_val, y_val),  # 驗證數據\n",
    "               callbacks=[model_cbk, model_mckp])  # Tensorboard回調函數紀錄訓練過程，ModelCheckpoint回調函數儲存最好的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()  # 查看history儲存的資訊有哪些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4c8c1ebe33e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mean square error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.ylim(0.02, 0.2)\n",
    "plt.title('Mean square error')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['mean_absolute_error'], label='train')\n",
    "plt.plot(history.history['val_mean_absolute_error'], label='validation')\n",
    "plt.ylim(0.12, 0.26)\n",
    "plt.title('Mean absolute error')\n",
    "plt.ylabel('metrics')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "測試數據的誤差百分比：用測試數據預測房屋價格並與答案計算誤差百分比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'lab2-logs/models/Best-model-1.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-951bd6533e41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 載入模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lab2-logs/models/Best-model-1.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# 先將房屋價格取出\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 標準化數據\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    221\u001b[0m   \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'lab2-logs/models/Best-model-1.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# 載入模型\n",
    "model = keras.models.load_model('lab2-logs/models/Best-model-1.h5')\n",
    "# 先將房屋價格取出\n",
    "y_test = np.array(test_data['price'])\n",
    "# 標準化數據\n",
    "test_data = (test_data - mean) / std\n",
    "# 將輸入數據存成Numpy 格式\n",
    "x_test = np.array(test_data.drop('price', axis='columns'))\n",
    "# 預測測試數據\n",
    "y_pred = model.predict(x_test)\n",
    "# 將預測結果轉換回來(因為訓練時的訓練目標也有經過標準化)\n",
    "y_pred = np.reshape(y_pred * std['price'] + mean['price'], y_test.shape)\n",
    "# 計算平均的誤差百分比\n",
    "percentage_error = np.mean(np.abs(y_test - y_pred)) / np.mean(y_test) * 100\n",
    "# 顯示誤差百分比\n",
    "print(\"Model_1 Percentage Error: {:.2f}%\".format(percentage_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard 可視化工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這行指令可以幫助我們直接在jupyter notebook上顯示TensorBoard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:9530\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f1fd04fd978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --port 9530 --logdir lab2-logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 實驗二：過擬合問題\n",
    "\n",
    "### 方法一、減少網路權重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12967 samples, validate on 4323 samples\n",
      "Epoch 1/300\n",
      "12967/12967 [==============================] - 1s 42us/sample - loss: 0.5637 - mean_absolute_error: 0.4461 - val_loss: 0.3118 - val_mean_absolute_error: 0.3467\n",
      "Epoch 2/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.2780 - mean_absolute_error: 0.3281 - val_loss: 0.2506 - val_mean_absolute_error: 0.3037\n",
      "Epoch 3/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.2415 - mean_absolute_error: 0.3046 - val_loss: 0.2276 - val_mean_absolute_error: 0.2885\n",
      "Epoch 4/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.2217 - mean_absolute_error: 0.2909 - val_loss: 0.2073 - val_mean_absolute_error: 0.2799\n",
      "Epoch 5/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.2080 - mean_absolute_error: 0.2820 - val_loss: 0.1948 - val_mean_absolute_error: 0.2725\n",
      "Epoch 6/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1973 - mean_absolute_error: 0.2737 - val_loss: 0.1878 - val_mean_absolute_error: 0.2610\n",
      "Epoch 7/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.1871 - mean_absolute_error: 0.2652 - val_loss: 0.1803 - val_mean_absolute_error: 0.2535\n",
      "Epoch 8/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.1780 - mean_absolute_error: 0.2579 - val_loss: 0.1706 - val_mean_absolute_error: 0.2477\n",
      "Epoch 9/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.1710 - mean_absolute_error: 0.2518 - val_loss: 0.1680 - val_mean_absolute_error: 0.2403\n",
      "Epoch 10/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.1621 - mean_absolute_error: 0.2437 - val_loss: 0.1588 - val_mean_absolute_error: 0.2399\n",
      "Epoch 11/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.1564 - mean_absolute_error: 0.2394 - val_loss: 0.1596 - val_mean_absolute_error: 0.2325\n",
      "Epoch 12/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.1513 - mean_absolute_error: 0.2352 - val_loss: 0.1549 - val_mean_absolute_error: 0.2265\n",
      "Epoch 13/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.1467 - mean_absolute_error: 0.2312 - val_loss: 0.1487 - val_mean_absolute_error: 0.2264\n",
      "Epoch 14/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1439 - mean_absolute_error: 0.2296 - val_loss: 0.1552 - val_mean_absolute_error: 0.2279\n",
      "Epoch 15/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.1409 - mean_absolute_error: 0.2299 - val_loss: 0.1440 - val_mean_absolute_error: 0.2226\n",
      "Epoch 16/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1370 - mean_absolute_error: 0.2243 - val_loss: 0.1474 - val_mean_absolute_error: 0.2236\n",
      "Epoch 17/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1361 - mean_absolute_error: 0.2242 - val_loss: 0.1438 - val_mean_absolute_error: 0.2244\n",
      "Epoch 18/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.1318 - mean_absolute_error: 0.2213 - val_loss: 0.1397 - val_mean_absolute_error: 0.2209\n",
      "Epoch 19/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1303 - mean_absolute_error: 0.2209 - val_loss: 0.1436 - val_mean_absolute_error: 0.2222\n",
      "Epoch 20/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.1277 - mean_absolute_error: 0.2187 - val_loss: 0.1472 - val_mean_absolute_error: 0.2174\n",
      "Epoch 21/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1248 - mean_absolute_error: 0.2162 - val_loss: 0.1358 - val_mean_absolute_error: 0.2162\n",
      "Epoch 22/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1238 - mean_absolute_error: 0.2142 - val_loss: 0.1377 - val_mean_absolute_error: 0.2179\n",
      "Epoch 23/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1216 - mean_absolute_error: 0.2133 - val_loss: 0.1468 - val_mean_absolute_error: 0.2226\n",
      "Epoch 24/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1217 - mean_absolute_error: 0.2135 - val_loss: 0.1388 - val_mean_absolute_error: 0.2168\n",
      "Epoch 25/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1187 - mean_absolute_error: 0.2113 - val_loss: 0.1454 - val_mean_absolute_error: 0.2163\n",
      "Epoch 26/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1194 - mean_absolute_error: 0.2116 - val_loss: 0.1379 - val_mean_absolute_error: 0.2133\n",
      "Epoch 27/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1178 - mean_absolute_error: 0.2106 - val_loss: 0.1357 - val_mean_absolute_error: 0.2127\n",
      "Epoch 28/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.1162 - mean_absolute_error: 0.2105 - val_loss: 0.1378 - val_mean_absolute_error: 0.2139\n",
      "Epoch 29/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1135 - mean_absolute_error: 0.2079 - val_loss: 0.1333 - val_mean_absolute_error: 0.2117\n",
      "Epoch 30/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1153 - mean_absolute_error: 0.2087 - val_loss: 0.1357 - val_mean_absolute_error: 0.2136\n",
      "Epoch 31/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1128 - mean_absolute_error: 0.2077 - val_loss: 0.1364 - val_mean_absolute_error: 0.2164\n",
      "Epoch 32/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1132 - mean_absolute_error: 0.2083 - val_loss: 0.1373 - val_mean_absolute_error: 0.2126\n",
      "Epoch 33/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.1126 - mean_absolute_error: 0.2071 - val_loss: 0.1305 - val_mean_absolute_error: 0.2096\n",
      "Epoch 34/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.1105 - mean_absolute_error: 0.2056 - val_loss: 0.1363 - val_mean_absolute_error: 0.2096\n",
      "Epoch 35/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.1094 - mean_absolute_error: 0.2048 - val_loss: 0.1369 - val_mean_absolute_error: 0.2125\n",
      "Epoch 36/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1108 - mean_absolute_error: 0.2065 - val_loss: 0.1346 - val_mean_absolute_error: 0.2103\n",
      "Epoch 37/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1084 - mean_absolute_error: 0.2031 - val_loss: 0.1315 - val_mean_absolute_error: 0.2082\n",
      "Epoch 38/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.1071 - mean_absolute_error: 0.2025 - val_loss: 0.1309 - val_mean_absolute_error: 0.2078\n",
      "Epoch 39/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.1084 - mean_absolute_error: 0.2045 - val_loss: 0.1305 - val_mean_absolute_error: 0.2107\n",
      "Epoch 40/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1052 - mean_absolute_error: 0.2019 - val_loss: 0.1291 - val_mean_absolute_error: 0.2086\n",
      "Epoch 41/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.1056 - mean_absolute_error: 0.2019 - val_loss: 0.1294 - val_mean_absolute_error: 0.2123\n",
      "Epoch 42/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1060 - mean_absolute_error: 0.2021 - val_loss: 0.1383 - val_mean_absolute_error: 0.2090\n",
      "Epoch 43/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.1034 - mean_absolute_error: 0.2004 - val_loss: 0.1360 - val_mean_absolute_error: 0.2063\n",
      "Epoch 44/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1040 - mean_absolute_error: 0.2014 - val_loss: 0.1313 - val_mean_absolute_error: 0.2075\n",
      "Epoch 45/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1039 - mean_absolute_error: 0.1997 - val_loss: 0.1331 - val_mean_absolute_error: 0.2077\n",
      "Epoch 46/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1030 - mean_absolute_error: 0.1995 - val_loss: 0.1229 - val_mean_absolute_error: 0.2058\n",
      "Epoch 47/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.1026 - mean_absolute_error: 0.1996 - val_loss: 0.1364 - val_mean_absolute_error: 0.2060\n",
      "Epoch 48/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1026 - mean_absolute_error: 0.1987 - val_loss: 0.1309 - val_mean_absolute_error: 0.2038\n",
      "Epoch 49/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.1009 - mean_absolute_error: 0.1982 - val_loss: 0.1344 - val_mean_absolute_error: 0.2060\n",
      "Epoch 50/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.1017 - mean_absolute_error: 0.1980 - val_loss: 0.1303 - val_mean_absolute_error: 0.2038\n",
      "Epoch 51/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.1008 - mean_absolute_error: 0.1970 - val_loss: 0.1297 - val_mean_absolute_error: 0.2059\n",
      "Epoch 52/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0997 - mean_absolute_error: 0.1983 - val_loss: 0.1281 - val_mean_absolute_error: 0.2038\n",
      "Epoch 53/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0992 - mean_absolute_error: 0.1963 - val_loss: 0.1447 - val_mean_absolute_error: 0.2075\n",
      "Epoch 54/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0985 - mean_absolute_error: 0.1959 - val_loss: 0.1271 - val_mean_absolute_error: 0.2064\n",
      "Epoch 55/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0992 - mean_absolute_error: 0.1967 - val_loss: 0.1249 - val_mean_absolute_error: 0.2048\n",
      "Epoch 56/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0984 - mean_absolute_error: 0.1961 - val_loss: 0.1270 - val_mean_absolute_error: 0.2005\n",
      "Epoch 57/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0970 - mean_absolute_error: 0.1946 - val_loss: 0.1259 - val_mean_absolute_error: 0.2017\n",
      "Epoch 58/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0967 - mean_absolute_error: 0.1940 - val_loss: 0.1277 - val_mean_absolute_error: 0.2044\n",
      "Epoch 59/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0963 - mean_absolute_error: 0.1951 - val_loss: 0.1335 - val_mean_absolute_error: 0.2063\n",
      "Epoch 60/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0963 - mean_absolute_error: 0.1945 - val_loss: 0.1296 - val_mean_absolute_error: 0.2038\n",
      "Epoch 61/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0983 - mean_absolute_error: 0.1947 - val_loss: 0.1240 - val_mean_absolute_error: 0.2019\n",
      "Epoch 62/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0955 - mean_absolute_error: 0.1954 - val_loss: 0.1265 - val_mean_absolute_error: 0.2015\n",
      "Epoch 63/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0954 - mean_absolute_error: 0.1933 - val_loss: 0.1251 - val_mean_absolute_error: 0.2035\n",
      "Epoch 64/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0975 - mean_absolute_error: 0.1945 - val_loss: 0.1331 - val_mean_absolute_error: 0.2025\n",
      "Epoch 65/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0944 - mean_absolute_error: 0.1923 - val_loss: 0.1311 - val_mean_absolute_error: 0.2023\n",
      "Epoch 66/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0943 - mean_absolute_error: 0.1926 - val_loss: 0.1346 - val_mean_absolute_error: 0.2025\n",
      "Epoch 67/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0951 - mean_absolute_error: 0.1943 - val_loss: 0.1334 - val_mean_absolute_error: 0.2103\n",
      "Epoch 68/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0943 - mean_absolute_error: 0.1925 - val_loss: 0.1383 - val_mean_absolute_error: 0.2028\n",
      "Epoch 69/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0939 - mean_absolute_error: 0.1923 - val_loss: 0.1256 - val_mean_absolute_error: 0.2007\n",
      "Epoch 70/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0940 - mean_absolute_error: 0.1928 - val_loss: 0.1302 - val_mean_absolute_error: 0.2034\n",
      "Epoch 71/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0926 - mean_absolute_error: 0.1926 - val_loss: 0.1271 - val_mean_absolute_error: 0.2034\n",
      "Epoch 72/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0932 - mean_absolute_error: 0.1916 - val_loss: 0.1296 - val_mean_absolute_error: 0.2089\n",
      "Epoch 73/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0931 - mean_absolute_error: 0.1923 - val_loss: 0.1279 - val_mean_absolute_error: 0.2022\n",
      "Epoch 74/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0923 - mean_absolute_error: 0.1911 - val_loss: 0.1282 - val_mean_absolute_error: 0.2024\n",
      "Epoch 75/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0924 - mean_absolute_error: 0.1924 - val_loss: 0.1265 - val_mean_absolute_error: 0.2016\n",
      "Epoch 76/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0920 - mean_absolute_error: 0.1925 - val_loss: 0.1246 - val_mean_absolute_error: 0.2011\n",
      "Epoch 77/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0925 - mean_absolute_error: 0.1928 - val_loss: 0.1218 - val_mean_absolute_error: 0.1996\n",
      "Epoch 78/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0916 - mean_absolute_error: 0.1913 - val_loss: 0.1274 - val_mean_absolute_error: 0.2025\n",
      "Epoch 79/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0915 - mean_absolute_error: 0.1914 - val_loss: 0.1334 - val_mean_absolute_error: 0.2002\n",
      "Epoch 80/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0897 - mean_absolute_error: 0.1905 - val_loss: 0.1227 - val_mean_absolute_error: 0.2017\n",
      "Epoch 81/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0903 - mean_absolute_error: 0.1915 - val_loss: 0.1221 - val_mean_absolute_error: 0.1987\n",
      "Epoch 82/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0901 - mean_absolute_error: 0.1906 - val_loss: 0.1271 - val_mean_absolute_error: 0.2027\n",
      "Epoch 83/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0899 - mean_absolute_error: 0.1901 - val_loss: 0.1235 - val_mean_absolute_error: 0.2077\n",
      "Epoch 84/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0898 - mean_absolute_error: 0.1900 - val_loss: 0.1331 - val_mean_absolute_error: 0.2005\n",
      "Epoch 85/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0902 - mean_absolute_error: 0.1900 - val_loss: 0.1196 - val_mean_absolute_error: 0.1996\n",
      "Epoch 86/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0893 - mean_absolute_error: 0.1903 - val_loss: 0.1279 - val_mean_absolute_error: 0.1977\n",
      "Epoch 87/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0893 - mean_absolute_error: 0.1893 - val_loss: 0.1335 - val_mean_absolute_error: 0.1998\n",
      "Epoch 88/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0896 - mean_absolute_error: 0.1897 - val_loss: 0.1203 - val_mean_absolute_error: 0.2008\n",
      "Epoch 89/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0889 - mean_absolute_error: 0.1897 - val_loss: 0.1283 - val_mean_absolute_error: 0.1999\n",
      "Epoch 90/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0880 - mean_absolute_error: 0.1891 - val_loss: 0.1220 - val_mean_absolute_error: 0.2014\n",
      "Epoch 91/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0891 - mean_absolute_error: 0.1894 - val_loss: 0.1371 - val_mean_absolute_error: 0.2110\n",
      "Epoch 92/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0881 - mean_absolute_error: 0.1890 - val_loss: 0.1258 - val_mean_absolute_error: 0.1980\n",
      "Epoch 93/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0880 - mean_absolute_error: 0.1894 - val_loss: 0.1320 - val_mean_absolute_error: 0.2034\n",
      "Epoch 94/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0871 - mean_absolute_error: 0.1891 - val_loss: 0.1243 - val_mean_absolute_error: 0.1976\n",
      "Epoch 95/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0872 - mean_absolute_error: 0.1886 - val_loss: 0.1251 - val_mean_absolute_error: 0.2002\n",
      "Epoch 96/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0870 - mean_absolute_error: 0.1878 - val_loss: 0.1237 - val_mean_absolute_error: 0.1975\n",
      "Epoch 97/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0863 - mean_absolute_error: 0.1885 - val_loss: 0.1224 - val_mean_absolute_error: 0.1986\n",
      "Epoch 98/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0869 - mean_absolute_error: 0.1889 - val_loss: 0.1234 - val_mean_absolute_error: 0.2042\n",
      "Epoch 99/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0872 - mean_absolute_error: 0.1881 - val_loss: 0.1191 - val_mean_absolute_error: 0.1971\n",
      "Epoch 100/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0864 - mean_absolute_error: 0.1876 - val_loss: 0.1324 - val_mean_absolute_error: 0.2103\n",
      "Epoch 101/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0869 - mean_absolute_error: 0.1886 - val_loss: 0.1241 - val_mean_absolute_error: 0.1983\n",
      "Epoch 102/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0853 - mean_absolute_error: 0.1862 - val_loss: 0.1197 - val_mean_absolute_error: 0.1978\n",
      "Epoch 103/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0867 - mean_absolute_error: 0.1867 - val_loss: 0.1201 - val_mean_absolute_error: 0.1967\n",
      "Epoch 104/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0865 - mean_absolute_error: 0.1880 - val_loss: 0.1254 - val_mean_absolute_error: 0.2003\n",
      "Epoch 105/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0854 - mean_absolute_error: 0.1869 - val_loss: 0.1185 - val_mean_absolute_error: 0.1972\n",
      "Epoch 106/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0853 - mean_absolute_error: 0.1864 - val_loss: 0.1248 - val_mean_absolute_error: 0.1990\n",
      "Epoch 107/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0860 - mean_absolute_error: 0.1876 - val_loss: 0.1209 - val_mean_absolute_error: 0.1960\n",
      "Epoch 108/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0848 - mean_absolute_error: 0.1863 - val_loss: 0.1290 - val_mean_absolute_error: 0.2006\n",
      "Epoch 109/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0849 - mean_absolute_error: 0.1877 - val_loss: 0.1280 - val_mean_absolute_error: 0.1996\n",
      "Epoch 110/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0854 - mean_absolute_error: 0.1873 - val_loss: 0.1211 - val_mean_absolute_error: 0.1950\n",
      "Epoch 111/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0835 - mean_absolute_error: 0.1856 - val_loss: 0.1224 - val_mean_absolute_error: 0.1987\n",
      "Epoch 112/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0838 - mean_absolute_error: 0.1860 - val_loss: 0.1322 - val_mean_absolute_error: 0.2004\n",
      "Epoch 113/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0849 - mean_absolute_error: 0.1861 - val_loss: 0.1183 - val_mean_absolute_error: 0.1956\n",
      "Epoch 114/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0841 - mean_absolute_error: 0.1858 - val_loss: 0.1174 - val_mean_absolute_error: 0.1971\n",
      "Epoch 115/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0843 - mean_absolute_error: 0.1847 - val_loss: 0.1215 - val_mean_absolute_error: 0.1992\n",
      "Epoch 116/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0844 - mean_absolute_error: 0.1864 - val_loss: 0.1247 - val_mean_absolute_error: 0.1993\n",
      "Epoch 117/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0833 - mean_absolute_error: 0.1864 - val_loss: 0.1162 - val_mean_absolute_error: 0.1955\n",
      "Epoch 118/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0836 - mean_absolute_error: 0.1858 - val_loss: 0.1205 - val_mean_absolute_error: 0.1980\n",
      "Epoch 119/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0827 - mean_absolute_error: 0.1842 - val_loss: 0.1208 - val_mean_absolute_error: 0.1979\n",
      "Epoch 120/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0840 - mean_absolute_error: 0.1853 - val_loss: 0.1221 - val_mean_absolute_error: 0.1991\n",
      "Epoch 121/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0828 - mean_absolute_error: 0.1850 - val_loss: 0.1212 - val_mean_absolute_error: 0.1972\n",
      "Epoch 122/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0815 - mean_absolute_error: 0.1841 - val_loss: 0.1186 - val_mean_absolute_error: 0.1978\n",
      "Epoch 123/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0838 - mean_absolute_error: 0.1860 - val_loss: 0.1289 - val_mean_absolute_error: 0.2022\n",
      "Epoch 124/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0822 - mean_absolute_error: 0.1857 - val_loss: 0.1291 - val_mean_absolute_error: 0.2070\n",
      "Epoch 125/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0820 - mean_absolute_error: 0.1842 - val_loss: 0.1235 - val_mean_absolute_error: 0.2006\n",
      "Epoch 126/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0825 - mean_absolute_error: 0.1851 - val_loss: 0.1249 - val_mean_absolute_error: 0.1952\n",
      "Epoch 127/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0812 - mean_absolute_error: 0.1840 - val_loss: 0.1208 - val_mean_absolute_error: 0.1959\n",
      "Epoch 128/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0817 - mean_absolute_error: 0.1846 - val_loss: 0.1263 - val_mean_absolute_error: 0.1977\n",
      "Epoch 129/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0807 - mean_absolute_error: 0.1842 - val_loss: 0.1242 - val_mean_absolute_error: 0.1989\n",
      "Epoch 130/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0813 - mean_absolute_error: 0.1843 - val_loss: 0.1259 - val_mean_absolute_error: 0.2028\n",
      "Epoch 131/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0813 - mean_absolute_error: 0.1843 - val_loss: 0.1213 - val_mean_absolute_error: 0.1987\n",
      "Epoch 132/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0800 - mean_absolute_error: 0.1831 - val_loss: 0.1252 - val_mean_absolute_error: 0.1971\n",
      "Epoch 133/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0807 - mean_absolute_error: 0.1840 - val_loss: 0.1200 - val_mean_absolute_error: 0.1954\n",
      "Epoch 134/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0803 - mean_absolute_error: 0.1829 - val_loss: 0.1277 - val_mean_absolute_error: 0.1976\n",
      "Epoch 135/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0813 - mean_absolute_error: 0.1849 - val_loss: 0.1188 - val_mean_absolute_error: 0.1946\n",
      "Epoch 136/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0791 - mean_absolute_error: 0.1826 - val_loss: 0.1270 - val_mean_absolute_error: 0.1943\n",
      "Epoch 137/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0800 - mean_absolute_error: 0.1833 - val_loss: 0.1252 - val_mean_absolute_error: 0.2019\n",
      "Epoch 138/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0800 - mean_absolute_error: 0.1832 - val_loss: 0.1186 - val_mean_absolute_error: 0.1952\n",
      "Epoch 139/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0797 - mean_absolute_error: 0.1830 - val_loss: 0.1200 - val_mean_absolute_error: 0.1938\n",
      "Epoch 140/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0792 - mean_absolute_error: 0.1820 - val_loss: 0.1216 - val_mean_absolute_error: 0.1954\n",
      "Epoch 141/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0796 - mean_absolute_error: 0.1830 - val_loss: 0.1290 - val_mean_absolute_error: 0.1994\n",
      "Epoch 142/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0797 - mean_absolute_error: 0.1827 - val_loss: 0.1156 - val_mean_absolute_error: 0.1950\n",
      "Epoch 143/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0793 - mean_absolute_error: 0.1827 - val_loss: 0.1212 - val_mean_absolute_error: 0.1942\n",
      "Epoch 144/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0787 - mean_absolute_error: 0.1822 - val_loss: 0.1240 - val_mean_absolute_error: 0.1951\n",
      "Epoch 145/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0793 - mean_absolute_error: 0.1822 - val_loss: 0.1217 - val_mean_absolute_error: 0.2007\n",
      "Epoch 146/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0793 - mean_absolute_error: 0.1826 - val_loss: 0.1309 - val_mean_absolute_error: 0.1958\n",
      "Epoch 147/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0788 - mean_absolute_error: 0.1823 - val_loss: 0.1238 - val_mean_absolute_error: 0.1930\n",
      "Epoch 148/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0812 - mean_absolute_error: 0.1839 - val_loss: 0.1195 - val_mean_absolute_error: 0.1931\n",
      "Epoch 149/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0788 - mean_absolute_error: 0.1809 - val_loss: 0.1213 - val_mean_absolute_error: 0.1936\n",
      "Epoch 150/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0784 - mean_absolute_error: 0.1817 - val_loss: 0.1163 - val_mean_absolute_error: 0.1923\n",
      "Epoch 151/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0784 - mean_absolute_error: 0.1822 - val_loss: 0.1234 - val_mean_absolute_error: 0.1963\n",
      "Epoch 152/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0781 - mean_absolute_error: 0.1815 - val_loss: 0.1261 - val_mean_absolute_error: 0.1933\n",
      "Epoch 153/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0770 - mean_absolute_error: 0.1805 - val_loss: 0.1257 - val_mean_absolute_error: 0.1982\n",
      "Epoch 154/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0787 - mean_absolute_error: 0.1825 - val_loss: 0.1175 - val_mean_absolute_error: 0.1943\n",
      "Epoch 155/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0790 - mean_absolute_error: 0.1823 - val_loss: 0.1246 - val_mean_absolute_error: 0.1927\n",
      "Epoch 156/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0778 - mean_absolute_error: 0.1810 - val_loss: 0.1275 - val_mean_absolute_error: 0.1985\n",
      "Epoch 157/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0785 - mean_absolute_error: 0.1816 - val_loss: 0.1169 - val_mean_absolute_error: 0.1929\n",
      "Epoch 158/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0774 - mean_absolute_error: 0.1810 - val_loss: 0.1269 - val_mean_absolute_error: 0.1974\n",
      "Epoch 159/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0799 - mean_absolute_error: 0.1824 - val_loss: 0.1249 - val_mean_absolute_error: 0.1985\n",
      "Epoch 160/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0783 - mean_absolute_error: 0.1825 - val_loss: 0.1229 - val_mean_absolute_error: 0.1930\n",
      "Epoch 161/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0778 - mean_absolute_error: 0.1814 - val_loss: 0.1237 - val_mean_absolute_error: 0.1939\n",
      "Epoch 162/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0775 - mean_absolute_error: 0.1805 - val_loss: 0.1239 - val_mean_absolute_error: 0.1932\n",
      "Epoch 163/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0786 - mean_absolute_error: 0.1822 - val_loss: 0.1201 - val_mean_absolute_error: 0.1973\n",
      "Epoch 164/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0782 - mean_absolute_error: 0.1815 - val_loss: 0.1295 - val_mean_absolute_error: 0.2007\n",
      "Epoch 165/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0773 - mean_absolute_error: 0.1805 - val_loss: 0.1272 - val_mean_absolute_error: 0.1972\n",
      "Epoch 166/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0772 - mean_absolute_error: 0.1813 - val_loss: 0.1261 - val_mean_absolute_error: 0.1962\n",
      "Epoch 167/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0765 - mean_absolute_error: 0.1803 - val_loss: 0.1273 - val_mean_absolute_error: 0.2005\n",
      "Epoch 168/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0773 - mean_absolute_error: 0.1814 - val_loss: 0.1277 - val_mean_absolute_error: 0.1984\n",
      "Epoch 169/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0777 - mean_absolute_error: 0.1810 - val_loss: 0.1207 - val_mean_absolute_error: 0.1943\n",
      "Epoch 170/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0772 - mean_absolute_error: 0.1820 - val_loss: 0.1217 - val_mean_absolute_error: 0.1961\n",
      "Epoch 171/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0778 - mean_absolute_error: 0.1807 - val_loss: 0.1197 - val_mean_absolute_error: 0.1937\n",
      "Epoch 172/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0763 - mean_absolute_error: 0.1803 - val_loss: 0.1226 - val_mean_absolute_error: 0.1929\n",
      "Epoch 173/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0771 - mean_absolute_error: 0.1804 - val_loss: 0.1242 - val_mean_absolute_error: 0.1944\n",
      "Epoch 174/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0782 - mean_absolute_error: 0.1820 - val_loss: 0.1252 - val_mean_absolute_error: 0.1945\n",
      "Epoch 175/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0764 - mean_absolute_error: 0.1801 - val_loss: 0.1264 - val_mean_absolute_error: 0.1944\n",
      "Epoch 176/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0775 - mean_absolute_error: 0.1819 - val_loss: 0.1253 - val_mean_absolute_error: 0.1981\n",
      "Epoch 177/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0766 - mean_absolute_error: 0.1809 - val_loss: 0.1206 - val_mean_absolute_error: 0.1945\n",
      "Epoch 178/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0768 - mean_absolute_error: 0.1798 - val_loss: 0.1269 - val_mean_absolute_error: 0.1968\n",
      "Epoch 179/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0764 - mean_absolute_error: 0.1795 - val_loss: 0.1174 - val_mean_absolute_error: 0.1924\n",
      "Epoch 180/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0765 - mean_absolute_error: 0.1802 - val_loss: 0.1254 - val_mean_absolute_error: 0.1941\n",
      "Epoch 181/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0762 - mean_absolute_error: 0.1804 - val_loss: 0.1245 - val_mean_absolute_error: 0.1938\n",
      "Epoch 182/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0760 - mean_absolute_error: 0.1795 - val_loss: 0.1257 - val_mean_absolute_error: 0.1948\n",
      "Epoch 183/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0759 - mean_absolute_error: 0.1799 - val_loss: 0.1280 - val_mean_absolute_error: 0.1950\n",
      "Epoch 184/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0769 - mean_absolute_error: 0.1802 - val_loss: 0.1255 - val_mean_absolute_error: 0.1949\n",
      "Epoch 185/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0758 - mean_absolute_error: 0.1804 - val_loss: 0.1251 - val_mean_absolute_error: 0.1946\n",
      "Epoch 186/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0769 - mean_absolute_error: 0.1808 - val_loss: 0.1349 - val_mean_absolute_error: 0.1995\n",
      "Epoch 187/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0769 - mean_absolute_error: 0.1811 - val_loss: 0.1245 - val_mean_absolute_error: 0.1936\n",
      "Epoch 188/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0765 - mean_absolute_error: 0.1807 - val_loss: 0.1254 - val_mean_absolute_error: 0.2043\n",
      "Epoch 189/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0765 - mean_absolute_error: 0.1799 - val_loss: 0.1227 - val_mean_absolute_error: 0.1935\n",
      "Epoch 190/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0757 - mean_absolute_error: 0.1791 - val_loss: 0.1224 - val_mean_absolute_error: 0.1947\n",
      "Epoch 191/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0757 - mean_absolute_error: 0.1799 - val_loss: 0.1283 - val_mean_absolute_error: 0.1991\n",
      "Epoch 192/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0760 - mean_absolute_error: 0.1795 - val_loss: 0.1244 - val_mean_absolute_error: 0.1933\n",
      "Epoch 193/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0750 - mean_absolute_error: 0.1787 - val_loss: 0.1259 - val_mean_absolute_error: 0.1948\n",
      "Epoch 194/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0776 - mean_absolute_error: 0.1820 - val_loss: 0.1213 - val_mean_absolute_error: 0.1934\n",
      "Epoch 195/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0761 - mean_absolute_error: 0.1801 - val_loss: 0.1292 - val_mean_absolute_error: 0.1957\n",
      "Epoch 196/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0771 - mean_absolute_error: 0.1808 - val_loss: 0.1250 - val_mean_absolute_error: 0.1999\n",
      "Epoch 197/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0756 - mean_absolute_error: 0.1801 - val_loss: 0.1291 - val_mean_absolute_error: 0.1980\n",
      "Epoch 198/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0767 - mean_absolute_error: 0.1815 - val_loss: 0.1287 - val_mean_absolute_error: 0.2052\n",
      "Epoch 199/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0756 - mean_absolute_error: 0.1792 - val_loss: 0.1219 - val_mean_absolute_error: 0.1983\n",
      "Epoch 200/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0751 - mean_absolute_error: 0.1795 - val_loss: 0.1207 - val_mean_absolute_error: 0.1951\n",
      "Epoch 201/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0758 - mean_absolute_error: 0.1792 - val_loss: 0.1234 - val_mean_absolute_error: 0.1918\n",
      "Epoch 202/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0766 - mean_absolute_error: 0.1802 - val_loss: 0.1277 - val_mean_absolute_error: 0.1997\n",
      "Epoch 203/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0754 - mean_absolute_error: 0.1783 - val_loss: 0.1257 - val_mean_absolute_error: 0.1944\n",
      "Epoch 204/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0760 - mean_absolute_error: 0.1804 - val_loss: 0.1243 - val_mean_absolute_error: 0.1978\n",
      "Epoch 205/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0751 - mean_absolute_error: 0.1792 - val_loss: 0.1264 - val_mean_absolute_error: 0.1939\n",
      "Epoch 206/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0758 - mean_absolute_error: 0.1789 - val_loss: 0.1219 - val_mean_absolute_error: 0.1935\n",
      "Epoch 207/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0750 - mean_absolute_error: 0.1791 - val_loss: 0.1233 - val_mean_absolute_error: 0.1957\n",
      "Epoch 208/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0739 - mean_absolute_error: 0.1779 - val_loss: 0.1272 - val_mean_absolute_error: 0.1951\n",
      "Epoch 209/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0746 - mean_absolute_error: 0.1784 - val_loss: 0.1239 - val_mean_absolute_error: 0.1952\n",
      "Epoch 210/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0774 - mean_absolute_error: 0.1800 - val_loss: 0.1241 - val_mean_absolute_error: 0.1957\n",
      "Epoch 211/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0764 - mean_absolute_error: 0.1799 - val_loss: 0.1353 - val_mean_absolute_error: 0.1970\n",
      "Epoch 212/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0756 - mean_absolute_error: 0.1784 - val_loss: 0.1264 - val_mean_absolute_error: 0.1948\n",
      "Epoch 213/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0744 - mean_absolute_error: 0.1784 - val_loss: 0.1217 - val_mean_absolute_error: 0.1921\n",
      "Epoch 214/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0745 - mean_absolute_error: 0.1786 - val_loss: 0.1237 - val_mean_absolute_error: 0.1944\n",
      "Epoch 215/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0755 - mean_absolute_error: 0.1797 - val_loss: 0.1246 - val_mean_absolute_error: 0.1961\n",
      "Epoch 216/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0747 - mean_absolute_error: 0.1784 - val_loss: 0.1270 - val_mean_absolute_error: 0.1935\n",
      "Epoch 217/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0749 - mean_absolute_error: 0.1789 - val_loss: 0.1317 - val_mean_absolute_error: 0.2024\n",
      "Epoch 218/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0736 - mean_absolute_error: 0.1781 - val_loss: 0.1280 - val_mean_absolute_error: 0.1967\n",
      "Epoch 219/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0738 - mean_absolute_error: 0.1778 - val_loss: 0.1202 - val_mean_absolute_error: 0.1945\n",
      "Epoch 220/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0745 - mean_absolute_error: 0.1777 - val_loss: 0.1242 - val_mean_absolute_error: 0.1990\n",
      "Epoch 221/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0763 - mean_absolute_error: 0.1797 - val_loss: 0.1198 - val_mean_absolute_error: 0.1924\n",
      "Epoch 222/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0742 - mean_absolute_error: 0.1783 - val_loss: 0.1286 - val_mean_absolute_error: 0.1935\n",
      "Epoch 223/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0746 - mean_absolute_error: 0.1787 - val_loss: 0.1273 - val_mean_absolute_error: 0.1965\n",
      "Epoch 224/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0739 - mean_absolute_error: 0.1785 - val_loss: 0.1331 - val_mean_absolute_error: 0.2025\n",
      "Epoch 225/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0740 - mean_absolute_error: 0.1775 - val_loss: 0.1257 - val_mean_absolute_error: 0.1957\n",
      "Epoch 226/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0746 - mean_absolute_error: 0.1783 - val_loss: 0.1284 - val_mean_absolute_error: 0.1957\n",
      "Epoch 227/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0743 - mean_absolute_error: 0.1783 - val_loss: 0.1252 - val_mean_absolute_error: 0.1947\n",
      "Epoch 228/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0744 - mean_absolute_error: 0.1784 - val_loss: 0.1261 - val_mean_absolute_error: 0.1940\n",
      "Epoch 229/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0734 - mean_absolute_error: 0.1775 - val_loss: 0.1265 - val_mean_absolute_error: 0.1975\n",
      "Epoch 230/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0748 - mean_absolute_error: 0.1790 - val_loss: 0.1241 - val_mean_absolute_error: 0.1984\n",
      "Epoch 231/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0733 - mean_absolute_error: 0.1775 - val_loss: 0.1293 - val_mean_absolute_error: 0.1982\n",
      "Epoch 232/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0736 - mean_absolute_error: 0.1771 - val_loss: 0.1265 - val_mean_absolute_error: 0.1982\n",
      "Epoch 233/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0746 - mean_absolute_error: 0.1794 - val_loss: 0.1335 - val_mean_absolute_error: 0.1982\n",
      "Epoch 234/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0746 - mean_absolute_error: 0.1783 - val_loss: 0.1261 - val_mean_absolute_error: 0.1940\n",
      "Epoch 235/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0745 - mean_absolute_error: 0.1788 - val_loss: 0.1281 - val_mean_absolute_error: 0.2038\n",
      "Epoch 236/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0744 - mean_absolute_error: 0.1784 - val_loss: 0.1301 - val_mean_absolute_error: 0.1941\n",
      "Epoch 237/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0743 - mean_absolute_error: 0.1791 - val_loss: 0.1243 - val_mean_absolute_error: 0.1938\n",
      "Epoch 238/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0742 - mean_absolute_error: 0.1780 - val_loss: 0.1252 - val_mean_absolute_error: 0.1929\n",
      "Epoch 239/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0753 - mean_absolute_error: 0.1794 - val_loss: 0.1238 - val_mean_absolute_error: 0.1932\n",
      "Epoch 240/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0750 - mean_absolute_error: 0.1786 - val_loss: 0.1230 - val_mean_absolute_error: 0.1942\n",
      "Epoch 241/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0740 - mean_absolute_error: 0.1779 - val_loss: 0.1288 - val_mean_absolute_error: 0.1998\n",
      "Epoch 242/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0743 - mean_absolute_error: 0.1788 - val_loss: 0.1219 - val_mean_absolute_error: 0.1962\n",
      "Epoch 243/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0745 - mean_absolute_error: 0.1786 - val_loss: 0.1211 - val_mean_absolute_error: 0.1931\n",
      "Epoch 244/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0732 - mean_absolute_error: 0.1775 - val_loss: 0.1247 - val_mean_absolute_error: 0.1930\n",
      "Epoch 245/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0738 - mean_absolute_error: 0.1778 - val_loss: 0.1261 - val_mean_absolute_error: 0.1929\n",
      "Epoch 246/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0742 - mean_absolute_error: 0.1782 - val_loss: 0.1281 - val_mean_absolute_error: 0.1943\n",
      "Epoch 247/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0728 - mean_absolute_error: 0.1764 - val_loss: 0.1280 - val_mean_absolute_error: 0.1932\n",
      "Epoch 248/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0740 - mean_absolute_error: 0.1778 - val_loss: 0.1235 - val_mean_absolute_error: 0.1941\n",
      "Epoch 249/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0748 - mean_absolute_error: 0.1780 - val_loss: 0.1264 - val_mean_absolute_error: 0.2003\n",
      "Epoch 250/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0730 - mean_absolute_error: 0.1771 - val_loss: 0.1295 - val_mean_absolute_error: 0.1964\n",
      "Epoch 251/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0726 - mean_absolute_error: 0.1770 - val_loss: 0.1255 - val_mean_absolute_error: 0.1984\n",
      "Epoch 252/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0737 - mean_absolute_error: 0.1779 - val_loss: 0.1363 - val_mean_absolute_error: 0.1992\n",
      "Epoch 253/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0739 - mean_absolute_error: 0.1778 - val_loss: 0.1303 - val_mean_absolute_error: 0.1948\n",
      "Epoch 254/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0742 - mean_absolute_error: 0.1773 - val_loss: 0.1269 - val_mean_absolute_error: 0.1941\n",
      "Epoch 255/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0729 - mean_absolute_error: 0.1779 - val_loss: 0.1259 - val_mean_absolute_error: 0.1920\n",
      "Epoch 256/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0726 - mean_absolute_error: 0.1767 - val_loss: 0.1326 - val_mean_absolute_error: 0.1949\n",
      "Epoch 257/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0726 - mean_absolute_error: 0.1769 - val_loss: 0.1228 - val_mean_absolute_error: 0.1945\n",
      "Epoch 258/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0727 - mean_absolute_error: 0.1768 - val_loss: 0.1246 - val_mean_absolute_error: 0.1929\n",
      "Epoch 259/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0738 - mean_absolute_error: 0.1770 - val_loss: 0.1255 - val_mean_absolute_error: 0.1931\n",
      "Epoch 260/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0733 - mean_absolute_error: 0.1771 - val_loss: 0.1260 - val_mean_absolute_error: 0.1935\n",
      "Epoch 261/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0729 - mean_absolute_error: 0.1772 - val_loss: 0.1192 - val_mean_absolute_error: 0.1923\n",
      "Epoch 262/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0732 - mean_absolute_error: 0.1776 - val_loss: 0.1311 - val_mean_absolute_error: 0.1944\n",
      "Epoch 263/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0724 - mean_absolute_error: 0.1770 - val_loss: 0.1255 - val_mean_absolute_error: 0.1979\n",
      "Epoch 264/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0732 - mean_absolute_error: 0.1771 - val_loss: 0.1227 - val_mean_absolute_error: 0.1957\n",
      "Epoch 265/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0734 - mean_absolute_error: 0.1780 - val_loss: 0.1251 - val_mean_absolute_error: 0.1924\n",
      "Epoch 266/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0734 - mean_absolute_error: 0.1779 - val_loss: 0.1312 - val_mean_absolute_error: 0.2052\n",
      "Epoch 267/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0734 - mean_absolute_error: 0.1781 - val_loss: 0.1280 - val_mean_absolute_error: 0.1942\n",
      "Epoch 268/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0724 - mean_absolute_error: 0.1769 - val_loss: 0.1298 - val_mean_absolute_error: 0.1976\n",
      "Epoch 269/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0726 - mean_absolute_error: 0.1772 - val_loss: 0.1253 - val_mean_absolute_error: 0.1977\n",
      "Epoch 270/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0733 - mean_absolute_error: 0.1778 - val_loss: 0.1317 - val_mean_absolute_error: 0.1965\n",
      "Epoch 271/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0734 - mean_absolute_error: 0.1776 - val_loss: 0.1233 - val_mean_absolute_error: 0.1930\n",
      "Epoch 272/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0736 - mean_absolute_error: 0.1776 - val_loss: 0.1241 - val_mean_absolute_error: 0.1984\n",
      "Epoch 273/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0742 - mean_absolute_error: 0.1786 - val_loss: 0.1238 - val_mean_absolute_error: 0.1957\n",
      "Epoch 274/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0732 - mean_absolute_error: 0.1765 - val_loss: 0.1250 - val_mean_absolute_error: 0.1945\n",
      "Epoch 275/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0731 - mean_absolute_error: 0.1769 - val_loss: 0.1261 - val_mean_absolute_error: 0.1937\n",
      "Epoch 276/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0726 - mean_absolute_error: 0.1765 - val_loss: 0.1288 - val_mean_absolute_error: 0.1958\n",
      "Epoch 277/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0749 - mean_absolute_error: 0.1798 - val_loss: 0.1245 - val_mean_absolute_error: 0.1972\n",
      "Epoch 278/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0724 - mean_absolute_error: 0.1772 - val_loss: 0.1278 - val_mean_absolute_error: 0.1949\n",
      "Epoch 279/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0728 - mean_absolute_error: 0.1778 - val_loss: 0.1251 - val_mean_absolute_error: 0.2001\n",
      "Epoch 280/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0729 - mean_absolute_error: 0.1773 - val_loss: 0.1237 - val_mean_absolute_error: 0.1930\n",
      "Epoch 281/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0728 - mean_absolute_error: 0.1766 - val_loss: 0.1232 - val_mean_absolute_error: 0.1938\n",
      "Epoch 282/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0722 - mean_absolute_error: 0.1768 - val_loss: 0.1284 - val_mean_absolute_error: 0.1965\n",
      "Epoch 283/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0726 - mean_absolute_error: 0.1764 - val_loss: 0.1261 - val_mean_absolute_error: 0.1954\n",
      "Epoch 284/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0728 - mean_absolute_error: 0.1779 - val_loss: 0.1219 - val_mean_absolute_error: 0.1945\n",
      "Epoch 285/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0722 - mean_absolute_error: 0.1767 - val_loss: 0.1266 - val_mean_absolute_error: 0.1945\n",
      "Epoch 286/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0725 - mean_absolute_error: 0.1775 - val_loss: 0.1252 - val_mean_absolute_error: 0.1952\n",
      "Epoch 287/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0734 - mean_absolute_error: 0.1777 - val_loss: 0.1333 - val_mean_absolute_error: 0.1971\n",
      "Epoch 288/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0721 - mean_absolute_error: 0.1762 - val_loss: 0.1267 - val_mean_absolute_error: 0.1934\n",
      "Epoch 289/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0724 - mean_absolute_error: 0.1774 - val_loss: 0.1299 - val_mean_absolute_error: 0.1954\n",
      "Epoch 290/300\n",
      "12967/12967 [==============================] - 0s 28us/sample - loss: 0.0726 - mean_absolute_error: 0.1764 - val_loss: 0.1272 - val_mean_absolute_error: 0.1938\n",
      "Epoch 291/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0726 - mean_absolute_error: 0.1766 - val_loss: 0.1323 - val_mean_absolute_error: 0.1995\n",
      "Epoch 292/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0734 - mean_absolute_error: 0.1777 - val_loss: 0.1253 - val_mean_absolute_error: 0.1941\n",
      "Epoch 293/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0727 - mean_absolute_error: 0.1768 - val_loss: 0.1235 - val_mean_absolute_error: 0.1949\n",
      "Epoch 294/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0716 - mean_absolute_error: 0.1758 - val_loss: 0.1258 - val_mean_absolute_error: 0.2006\n",
      "Epoch 295/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0718 - mean_absolute_error: 0.1768 - val_loss: 0.1283 - val_mean_absolute_error: 0.2002\n",
      "Epoch 296/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0717 - mean_absolute_error: 0.1765 - val_loss: 0.1273 - val_mean_absolute_error: 0.1952\n",
      "Epoch 297/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0721 - mean_absolute_error: 0.1771 - val_loss: 0.1257 - val_mean_absolute_error: 0.1978\n",
      "Epoch 298/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0729 - mean_absolute_error: 0.1771 - val_loss: 0.1310 - val_mean_absolute_error: 0.2004\n",
      "Epoch 299/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0714 - mean_absolute_error: 0.1760 - val_loss: 0.1311 - val_mean_absolute_error: 0.1991\n",
      "Epoch 300/300\n",
      "12967/12967 [==============================] - 0s 29us/sample - loss: 0.0734 - mean_absolute_error: 0.1766 - val_loss: 0.1243 - val_mean_absolute_error: 0.1931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f45d03ccfd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = keras.Sequential(name='model-2')\n",
    "model_2.add(layers.Dense(16, activation='relu', input_shape=(21,)))\n",
    "model_2.add(layers.Dense(16, activation='relu'))\n",
    "model_2.add(layers.Dense(1))\n",
    "\n",
    "model_2.compile(keras.optimizers.Adam(0.001),\n",
    "                loss=keras.losses.MeanSquaredError(),\n",
    "                metrics=[keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "log_dir = os.path.join('lab2-logs', 'model-2')\n",
    "model_cbk = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "model_mckp = keras.callbacks.ModelCheckpoint(model_dir + '/Best-model-2.h5', \n",
    "                                             monitor='val_mean_absolute_error', \n",
    "                                             save_best_only=True, \n",
    "                                             mode='min')\n",
    "model_2.fit(x_train, y_train, \n",
    "            batch_size=64 ,\n",
    "            epochs=300, \n",
    "            validation_data=(x_val, y_val), \n",
    "            callbacks=[model_cbk, model_mckp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加入L1或L2 正則化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12967 samples, validate on 4323 samples\n",
      "Epoch 1/300\n",
      "12967/12967 [==============================] - 1s 52us/sample - loss: 0.3835 - mean_absolute_error: 0.3362 - val_loss: 0.2921 - val_mean_absolute_error: 0.2824\n",
      "Epoch 2/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.2892 - mean_absolute_error: 0.2813 - val_loss: 0.2878 - val_mean_absolute_error: 0.2684\n",
      "Epoch 3/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.2592 - mean_absolute_error: 0.2659 - val_loss: 0.2510 - val_mean_absolute_error: 0.2648\n",
      "Epoch 4/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.2401 - mean_absolute_error: 0.2571 - val_loss: 0.2331 - val_mean_absolute_error: 0.2509\n",
      "Epoch 5/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.2236 - mean_absolute_error: 0.2473 - val_loss: 0.2209 - val_mean_absolute_error: 0.2390\n",
      "Epoch 6/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.2083 - mean_absolute_error: 0.2347 - val_loss: 0.2131 - val_mean_absolute_error: 0.2382\n",
      "Epoch 7/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.1994 - mean_absolute_error: 0.2293 - val_loss: 0.2019 - val_mean_absolute_error: 0.2248\n",
      "Epoch 8/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.1881 - mean_absolute_error: 0.2213 - val_loss: 0.1959 - val_mean_absolute_error: 0.2205\n",
      "Epoch 9/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.1790 - mean_absolute_error: 0.2155 - val_loss: 0.1945 - val_mean_absolute_error: 0.2150\n",
      "Epoch 10/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.1727 - mean_absolute_error: 0.2110 - val_loss: 0.1832 - val_mean_absolute_error: 0.2122\n",
      "Epoch 11/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.1657 - mean_absolute_error: 0.2070 - val_loss: 0.1763 - val_mean_absolute_error: 0.2048\n",
      "Epoch 12/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1625 - mean_absolute_error: 0.2078 - val_loss: 0.1923 - val_mean_absolute_error: 0.2224\n",
      "Epoch 13/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1610 - mean_absolute_error: 0.2057 - val_loss: 0.1783 - val_mean_absolute_error: 0.2086\n",
      "Epoch 14/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1523 - mean_absolute_error: 0.1991 - val_loss: 0.1652 - val_mean_absolute_error: 0.2078\n",
      "Epoch 15/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1490 - mean_absolute_error: 0.1996 - val_loss: 0.1770 - val_mean_absolute_error: 0.2117\n",
      "Epoch 16/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.1476 - mean_absolute_error: 0.1967 - val_loss: 0.1612 - val_mean_absolute_error: 0.2032\n",
      "Epoch 17/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.1452 - mean_absolute_error: 0.1971 - val_loss: 0.1604 - val_mean_absolute_error: 0.2032\n",
      "Epoch 18/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.1397 - mean_absolute_error: 0.1934 - val_loss: 0.1594 - val_mean_absolute_error: 0.2004\n",
      "Epoch 19/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.1366 - mean_absolute_error: 0.1918 - val_loss: 0.1521 - val_mean_absolute_error: 0.1933\n",
      "Epoch 20/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1336 - mean_absolute_error: 0.1901 - val_loss: 0.1545 - val_mean_absolute_error: 0.1969\n",
      "Epoch 21/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.1344 - mean_absolute_error: 0.1922 - val_loss: 0.1609 - val_mean_absolute_error: 0.2076\n",
      "Epoch 22/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1342 - mean_absolute_error: 0.1920 - val_loss: 0.1505 - val_mean_absolute_error: 0.1954\n",
      "Epoch 23/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1280 - mean_absolute_error: 0.1875 - val_loss: 0.1565 - val_mean_absolute_error: 0.2011\n",
      "Epoch 24/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1294 - mean_absolute_error: 0.1897 - val_loss: 0.1416 - val_mean_absolute_error: 0.1934\n",
      "Epoch 25/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.1249 - mean_absolute_error: 0.1861 - val_loss: 0.1528 - val_mean_absolute_error: 0.1981\n",
      "Epoch 26/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1261 - mean_absolute_error: 0.1858 - val_loss: 0.1455 - val_mean_absolute_error: 0.1940\n",
      "Epoch 27/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1268 - mean_absolute_error: 0.1887 - val_loss: 0.1478 - val_mean_absolute_error: 0.1987\n",
      "Epoch 28/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1253 - mean_absolute_error: 0.1881 - val_loss: 0.1439 - val_mean_absolute_error: 0.1953\n",
      "Epoch 29/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.1219 - mean_absolute_error: 0.1841 - val_loss: 0.1385 - val_mean_absolute_error: 0.1870\n",
      "Epoch 30/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1163 - mean_absolute_error: 0.1820 - val_loss: 0.1526 - val_mean_absolute_error: 0.2000\n",
      "Epoch 31/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1172 - mean_absolute_error: 0.1836 - val_loss: 0.1460 - val_mean_absolute_error: 0.1977\n",
      "Epoch 32/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1223 - mean_absolute_error: 0.1847 - val_loss: 0.1364 - val_mean_absolute_error: 0.1908\n",
      "Epoch 33/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1170 - mean_absolute_error: 0.1825 - val_loss: 0.1401 - val_mean_absolute_error: 0.1884\n",
      "Epoch 34/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1157 - mean_absolute_error: 0.1828 - val_loss: 0.1360 - val_mean_absolute_error: 0.1886\n",
      "Epoch 35/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1170 - mean_absolute_error: 0.1831 - val_loss: 0.1432 - val_mean_absolute_error: 0.1892\n",
      "Epoch 36/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1126 - mean_absolute_error: 0.1795 - val_loss: 0.1313 - val_mean_absolute_error: 0.1832\n",
      "Epoch 37/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1159 - mean_absolute_error: 0.1836 - val_loss: 0.1414 - val_mean_absolute_error: 0.1987\n",
      "Epoch 38/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1127 - mean_absolute_error: 0.1796 - val_loss: 0.1343 - val_mean_absolute_error: 0.1891\n",
      "Epoch 39/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1099 - mean_absolute_error: 0.1790 - val_loss: 0.1361 - val_mean_absolute_error: 0.1854\n",
      "Epoch 40/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1139 - mean_absolute_error: 0.1825 - val_loss: 0.1555 - val_mean_absolute_error: 0.1885\n",
      "Epoch 41/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1078 - mean_absolute_error: 0.1778 - val_loss: 0.1328 - val_mean_absolute_error: 0.1929\n",
      "Epoch 42/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1104 - mean_absolute_error: 0.1817 - val_loss: 0.1344 - val_mean_absolute_error: 0.1891\n",
      "Epoch 43/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1089 - mean_absolute_error: 0.1777 - val_loss: 0.1342 - val_mean_absolute_error: 0.1877\n",
      "Epoch 44/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1063 - mean_absolute_error: 0.1775 - val_loss: 0.1332 - val_mean_absolute_error: 0.1891\n",
      "Epoch 45/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.1071 - mean_absolute_error: 0.1792 - val_loss: 0.1367 - val_mean_absolute_error: 0.1899\n",
      "Epoch 46/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1030 - mean_absolute_error: 0.1756 - val_loss: 0.1370 - val_mean_absolute_error: 0.1854\n",
      "Epoch 47/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1054 - mean_absolute_error: 0.1778 - val_loss: 0.1403 - val_mean_absolute_error: 0.1861\n",
      "Epoch 48/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1051 - mean_absolute_error: 0.1766 - val_loss: 0.1302 - val_mean_absolute_error: 0.1837\n",
      "Epoch 49/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1036 - mean_absolute_error: 0.1760 - val_loss: 0.1398 - val_mean_absolute_error: 0.1882\n",
      "Epoch 50/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.1030 - mean_absolute_error: 0.1768 - val_loss: 0.1275 - val_mean_absolute_error: 0.1831\n",
      "Epoch 51/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1036 - mean_absolute_error: 0.1777 - val_loss: 0.1296 - val_mean_absolute_error: 0.1841\n",
      "Epoch 52/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1043 - mean_absolute_error: 0.1788 - val_loss: 0.1682 - val_mean_absolute_error: 0.2117\n",
      "Epoch 53/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1030 - mean_absolute_error: 0.1784 - val_loss: 0.1312 - val_mean_absolute_error: 0.1911\n",
      "Epoch 54/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1017 - mean_absolute_error: 0.1747 - val_loss: 0.1289 - val_mean_absolute_error: 0.1844\n",
      "Epoch 55/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0999 - mean_absolute_error: 0.1748 - val_loss: 0.1268 - val_mean_absolute_error: 0.1834\n",
      "Epoch 56/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.0984 - mean_absolute_error: 0.1740 - val_loss: 0.1365 - val_mean_absolute_error: 0.1857\n",
      "Epoch 57/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1007 - mean_absolute_error: 0.1738 - val_loss: 0.1372 - val_mean_absolute_error: 0.1948\n",
      "Epoch 58/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.1018 - mean_absolute_error: 0.1774 - val_loss: 0.1290 - val_mean_absolute_error: 0.1894\n",
      "Epoch 59/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0994 - mean_absolute_error: 0.1744 - val_loss: 0.1284 - val_mean_absolute_error: 0.1834\n",
      "Epoch 60/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0972 - mean_absolute_error: 0.1732 - val_loss: 0.1456 - val_mean_absolute_error: 0.1907\n",
      "Epoch 61/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0967 - mean_absolute_error: 0.1726 - val_loss: 0.1246 - val_mean_absolute_error: 0.1854\n",
      "Epoch 62/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0961 - mean_absolute_error: 0.1731 - val_loss: 0.1299 - val_mean_absolute_error: 0.1834\n",
      "Epoch 63/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0973 - mean_absolute_error: 0.1736 - val_loss: 0.1366 - val_mean_absolute_error: 0.1894\n",
      "Epoch 64/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.0967 - mean_absolute_error: 0.1742 - val_loss: 0.1244 - val_mean_absolute_error: 0.1825\n",
      "Epoch 65/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0987 - mean_absolute_error: 0.1732 - val_loss: 0.1307 - val_mean_absolute_error: 0.1876\n",
      "Epoch 66/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0938 - mean_absolute_error: 0.1719 - val_loss: 0.1254 - val_mean_absolute_error: 0.1840\n",
      "Epoch 67/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0961 - mean_absolute_error: 0.1723 - val_loss: 0.1357 - val_mean_absolute_error: 0.1843\n",
      "Epoch 68/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0938 - mean_absolute_error: 0.1705 - val_loss: 0.1533 - val_mean_absolute_error: 0.1915\n",
      "Epoch 69/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0958 - mean_absolute_error: 0.1724 - val_loss: 0.1252 - val_mean_absolute_error: 0.1858\n",
      "Epoch 70/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0947 - mean_absolute_error: 0.1727 - val_loss: 0.1325 - val_mean_absolute_error: 0.1942\n",
      "Epoch 71/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0935 - mean_absolute_error: 0.1720 - val_loss: 0.1339 - val_mean_absolute_error: 0.1880\n",
      "Epoch 72/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0938 - mean_absolute_error: 0.1712 - val_loss: 0.1352 - val_mean_absolute_error: 0.1909\n",
      "Epoch 73/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0953 - mean_absolute_error: 0.1727 - val_loss: 0.1372 - val_mean_absolute_error: 0.1856\n",
      "Epoch 74/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0943 - mean_absolute_error: 0.1711 - val_loss: 0.1272 - val_mean_absolute_error: 0.1966\n",
      "Epoch 75/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0950 - mean_absolute_error: 0.1736 - val_loss: 0.1246 - val_mean_absolute_error: 0.1860\n",
      "Epoch 76/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0919 - mean_absolute_error: 0.1706 - val_loss: 0.1413 - val_mean_absolute_error: 0.1892\n",
      "Epoch 77/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0906 - mean_absolute_error: 0.1703 - val_loss: 0.1293 - val_mean_absolute_error: 0.1864\n",
      "Epoch 78/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0915 - mean_absolute_error: 0.1693 - val_loss: 0.1306 - val_mean_absolute_error: 0.1909\n",
      "Epoch 79/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0932 - mean_absolute_error: 0.1708 - val_loss: 0.1292 - val_mean_absolute_error: 0.1836\n",
      "Epoch 80/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0926 - mean_absolute_error: 0.1708 - val_loss: 0.1299 - val_mean_absolute_error: 0.1927\n",
      "Epoch 81/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0910 - mean_absolute_error: 0.1704 - val_loss: 0.1238 - val_mean_absolute_error: 0.1846\n",
      "Epoch 82/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.0897 - mean_absolute_error: 0.1688 - val_loss: 0.1237 - val_mean_absolute_error: 0.1822\n",
      "Epoch 83/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0894 - mean_absolute_error: 0.1686 - val_loss: 0.1371 - val_mean_absolute_error: 0.1910\n",
      "Epoch 84/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0931 - mean_absolute_error: 0.1718 - val_loss: 0.1354 - val_mean_absolute_error: 0.1882\n",
      "Epoch 85/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0914 - mean_absolute_error: 0.1706 - val_loss: 0.1308 - val_mean_absolute_error: 0.1881\n",
      "Epoch 86/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0945 - mean_absolute_error: 0.1741 - val_loss: 0.1232 - val_mean_absolute_error: 0.1840\n",
      "Epoch 87/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0897 - mean_absolute_error: 0.1687 - val_loss: 0.1279 - val_mean_absolute_error: 0.1850\n",
      "Epoch 88/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0877 - mean_absolute_error: 0.1673 - val_loss: 0.1235 - val_mean_absolute_error: 0.1845\n",
      "Epoch 89/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0880 - mean_absolute_error: 0.1682 - val_loss: 0.1278 - val_mean_absolute_error: 0.1840\n",
      "Epoch 90/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0876 - mean_absolute_error: 0.1682 - val_loss: 0.1247 - val_mean_absolute_error: 0.1824\n",
      "Epoch 91/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0874 - mean_absolute_error: 0.1671 - val_loss: 0.1280 - val_mean_absolute_error: 0.1907\n",
      "Epoch 92/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0905 - mean_absolute_error: 0.1708 - val_loss: 0.1219 - val_mean_absolute_error: 0.1836\n",
      "Epoch 93/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.0879 - mean_absolute_error: 0.1684 - val_loss: 0.1189 - val_mean_absolute_error: 0.1802\n",
      "Epoch 94/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0883 - mean_absolute_error: 0.1688 - val_loss: 0.1264 - val_mean_absolute_error: 0.1855\n",
      "Epoch 95/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0881 - mean_absolute_error: 0.1685 - val_loss: 0.1235 - val_mean_absolute_error: 0.1854\n",
      "Epoch 96/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0947 - mean_absolute_error: 0.1738 - val_loss: 0.1242 - val_mean_absolute_error: 0.1892\n",
      "Epoch 97/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0893 - mean_absolute_error: 0.1686 - val_loss: 0.1242 - val_mean_absolute_error: 0.1836\n",
      "Epoch 98/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0889 - mean_absolute_error: 0.1688 - val_loss: 0.1311 - val_mean_absolute_error: 0.1859\n",
      "Epoch 99/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0868 - mean_absolute_error: 0.1672 - val_loss: 0.1290 - val_mean_absolute_error: 0.1883\n",
      "Epoch 100/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0874 - mean_absolute_error: 0.1672 - val_loss: 0.1451 - val_mean_absolute_error: 0.2085\n",
      "Epoch 101/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0892 - mean_absolute_error: 0.1686 - val_loss: 0.1277 - val_mean_absolute_error: 0.1860\n",
      "Epoch 102/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0853 - mean_absolute_error: 0.1674 - val_loss: 0.1286 - val_mean_absolute_error: 0.1853\n",
      "Epoch 103/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0876 - mean_absolute_error: 0.1673 - val_loss: 0.1195 - val_mean_absolute_error: 0.1822\n",
      "Epoch 104/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0846 - mean_absolute_error: 0.1658 - val_loss: 0.1408 - val_mean_absolute_error: 0.1888\n",
      "Epoch 105/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0852 - mean_absolute_error: 0.1671 - val_loss: 0.1291 - val_mean_absolute_error: 0.1877\n",
      "Epoch 106/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0900 - mean_absolute_error: 0.1692 - val_loss: 0.1412 - val_mean_absolute_error: 0.1959\n",
      "Epoch 107/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0868 - mean_absolute_error: 0.1685 - val_loss: 0.1199 - val_mean_absolute_error: 0.1818\n",
      "Epoch 108/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0866 - mean_absolute_error: 0.1672 - val_loss: 0.1283 - val_mean_absolute_error: 0.1941\n",
      "Epoch 109/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0859 - mean_absolute_error: 0.1669 - val_loss: 0.1208 - val_mean_absolute_error: 0.1827\n",
      "Epoch 110/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0844 - mean_absolute_error: 0.1657 - val_loss: 0.1198 - val_mean_absolute_error: 0.1871\n",
      "Epoch 111/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0877 - mean_absolute_error: 0.1692 - val_loss: 0.1240 - val_mean_absolute_error: 0.1841\n",
      "Epoch 112/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0860 - mean_absolute_error: 0.1666 - val_loss: 0.1313 - val_mean_absolute_error: 0.1942\n",
      "Epoch 113/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0867 - mean_absolute_error: 0.1672 - val_loss: 0.1383 - val_mean_absolute_error: 0.1887\n",
      "Epoch 114/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0863 - mean_absolute_error: 0.1680 - val_loss: 0.1267 - val_mean_absolute_error: 0.1843\n",
      "Epoch 115/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0840 - mean_absolute_error: 0.1658 - val_loss: 0.1222 - val_mean_absolute_error: 0.1820\n",
      "Epoch 116/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0824 - mean_absolute_error: 0.1652 - val_loss: 0.1222 - val_mean_absolute_error: 0.1881\n",
      "Epoch 117/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0841 - mean_absolute_error: 0.1650 - val_loss: 0.1294 - val_mean_absolute_error: 0.1864\n",
      "Epoch 118/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0862 - mean_absolute_error: 0.1674 - val_loss: 0.1335 - val_mean_absolute_error: 0.1900\n",
      "Epoch 119/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.0880 - mean_absolute_error: 0.1700 - val_loss: 0.1152 - val_mean_absolute_error: 0.1802\n",
      "Epoch 120/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0860 - mean_absolute_error: 0.1670 - val_loss: 0.1275 - val_mean_absolute_error: 0.1855\n",
      "Epoch 121/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0839 - mean_absolute_error: 0.1658 - val_loss: 0.1218 - val_mean_absolute_error: 0.1845\n",
      "Epoch 122/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0805 - mean_absolute_error: 0.1631 - val_loss: 0.1186 - val_mean_absolute_error: 0.1823\n",
      "Epoch 123/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0824 - mean_absolute_error: 0.1644 - val_loss: 0.1275 - val_mean_absolute_error: 0.1842\n",
      "Epoch 124/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0818 - mean_absolute_error: 0.1636 - val_loss: 0.1316 - val_mean_absolute_error: 0.1873\n",
      "Epoch 125/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0830 - mean_absolute_error: 0.1649 - val_loss: 0.1243 - val_mean_absolute_error: 0.1838\n",
      "Epoch 126/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0825 - mean_absolute_error: 0.1649 - val_loss: 0.1280 - val_mean_absolute_error: 0.1829\n",
      "Epoch 127/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0831 - mean_absolute_error: 0.1656 - val_loss: 0.1194 - val_mean_absolute_error: 0.1818\n",
      "Epoch 128/300\n",
      "12967/12967 [==============================] - 0s 33us/sample - loss: 0.0863 - mean_absolute_error: 0.1669 - val_loss: 0.1237 - val_mean_absolute_error: 0.1859\n",
      "Epoch 129/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0822 - mean_absolute_error: 0.1642 - val_loss: 0.1230 - val_mean_absolute_error: 0.1816\n",
      "Epoch 130/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0804 - mean_absolute_error: 0.1637 - val_loss: 0.1236 - val_mean_absolute_error: 0.1886\n",
      "Epoch 131/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.0809 - mean_absolute_error: 0.1629 - val_loss: 0.1315 - val_mean_absolute_error: 0.1900\n",
      "Epoch 132/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0849 - mean_absolute_error: 0.1667 - val_loss: 0.1300 - val_mean_absolute_error: 0.1878\n",
      "Epoch 133/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0817 - mean_absolute_error: 0.1645 - val_loss: 0.1432 - val_mean_absolute_error: 0.1879\n",
      "Epoch 134/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0878 - mean_absolute_error: 0.1680 - val_loss: 0.1466 - val_mean_absolute_error: 0.1949\n",
      "Epoch 135/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0890 - mean_absolute_error: 0.1687 - val_loss: 0.1210 - val_mean_absolute_error: 0.1812\n",
      "Epoch 136/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0813 - mean_absolute_error: 0.1636 - val_loss: 0.1228 - val_mean_absolute_error: 0.1839\n",
      "Epoch 137/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0787 - mean_absolute_error: 0.1620 - val_loss: 0.1150 - val_mean_absolute_error: 0.1819\n",
      "Epoch 138/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0816 - mean_absolute_error: 0.1648 - val_loss: 0.1234 - val_mean_absolute_error: 0.1845\n",
      "Epoch 139/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0812 - mean_absolute_error: 0.1634 - val_loss: 0.1334 - val_mean_absolute_error: 0.1828\n",
      "Epoch 140/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0813 - mean_absolute_error: 0.1632 - val_loss: 0.1237 - val_mean_absolute_error: 0.1874\n",
      "Epoch 141/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0802 - mean_absolute_error: 0.1627 - val_loss: 0.1191 - val_mean_absolute_error: 0.1818\n",
      "Epoch 142/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0796 - mean_absolute_error: 0.1623 - val_loss: 0.1308 - val_mean_absolute_error: 0.1847\n",
      "Epoch 143/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0822 - mean_absolute_error: 0.1639 - val_loss: 0.1252 - val_mean_absolute_error: 0.1893\n",
      "Epoch 144/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0830 - mean_absolute_error: 0.1656 - val_loss: 0.1283 - val_mean_absolute_error: 0.1870\n",
      "Epoch 145/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0829 - mean_absolute_error: 0.1655 - val_loss: 0.1334 - val_mean_absolute_error: 0.1847\n",
      "Epoch 146/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0805 - mean_absolute_error: 0.1637 - val_loss: 0.1235 - val_mean_absolute_error: 0.1851\n",
      "Epoch 147/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0807 - mean_absolute_error: 0.1630 - val_loss: 0.1227 - val_mean_absolute_error: 0.1892\n",
      "Epoch 148/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0832 - mean_absolute_error: 0.1658 - val_loss: 0.1244 - val_mean_absolute_error: 0.1824\n",
      "Epoch 149/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0830 - mean_absolute_error: 0.1650 - val_loss: 0.1349 - val_mean_absolute_error: 0.1880\n",
      "Epoch 150/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0821 - mean_absolute_error: 0.1655 - val_loss: 0.1209 - val_mean_absolute_error: 0.1827\n",
      "Epoch 151/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0792 - mean_absolute_error: 0.1620 - val_loss: 0.1267 - val_mean_absolute_error: 0.1837\n",
      "Epoch 152/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0784 - mean_absolute_error: 0.1617 - val_loss: 0.1371 - val_mean_absolute_error: 0.1899\n",
      "Epoch 153/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0800 - mean_absolute_error: 0.1635 - val_loss: 0.1250 - val_mean_absolute_error: 0.1846\n",
      "Epoch 154/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0804 - mean_absolute_error: 0.1629 - val_loss: 0.1143 - val_mean_absolute_error: 0.1812\n",
      "Epoch 155/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0814 - mean_absolute_error: 0.1630 - val_loss: 0.1317 - val_mean_absolute_error: 0.1925\n",
      "Epoch 156/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0812 - mean_absolute_error: 0.1632 - val_loss: 0.1196 - val_mean_absolute_error: 0.1834\n",
      "Epoch 157/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0787 - mean_absolute_error: 0.1623 - val_loss: 0.1202 - val_mean_absolute_error: 0.1834\n",
      "Epoch 158/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0803 - mean_absolute_error: 0.1638 - val_loss: 0.1273 - val_mean_absolute_error: 0.1849\n",
      "Epoch 159/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0792 - mean_absolute_error: 0.1624 - val_loss: 0.1241 - val_mean_absolute_error: 0.1840\n",
      "Epoch 160/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0784 - mean_absolute_error: 0.1619 - val_loss: 0.1269 - val_mean_absolute_error: 0.1869\n",
      "Epoch 161/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0800 - mean_absolute_error: 0.1623 - val_loss: 0.1238 - val_mean_absolute_error: 0.1892\n",
      "Epoch 162/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0800 - mean_absolute_error: 0.1633 - val_loss: 0.1284 - val_mean_absolute_error: 0.1876\n",
      "Epoch 163/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0801 - mean_absolute_error: 0.1636 - val_loss: 0.1201 - val_mean_absolute_error: 0.1838\n",
      "Epoch 164/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0814 - mean_absolute_error: 0.1642 - val_loss: 0.1256 - val_mean_absolute_error: 0.1869\n",
      "Epoch 165/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0835 - mean_absolute_error: 0.1655 - val_loss: 0.1311 - val_mean_absolute_error: 0.1903\n",
      "Epoch 166/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0776 - mean_absolute_error: 0.1613 - val_loss: 0.1271 - val_mean_absolute_error: 0.1838\n",
      "Epoch 167/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0771 - mean_absolute_error: 0.1608 - val_loss: 0.1346 - val_mean_absolute_error: 0.1875\n",
      "Epoch 168/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0819 - mean_absolute_error: 0.1652 - val_loss: 0.1154 - val_mean_absolute_error: 0.1804\n",
      "Epoch 169/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0827 - mean_absolute_error: 0.1653 - val_loss: 0.1205 - val_mean_absolute_error: 0.1858\n",
      "Epoch 170/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0787 - mean_absolute_error: 0.1615 - val_loss: 0.1228 - val_mean_absolute_error: 0.1822\n",
      "Epoch 171/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.0794 - mean_absolute_error: 0.1620 - val_loss: 0.1216 - val_mean_absolute_error: 0.1800\n",
      "Epoch 172/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0761 - mean_absolute_error: 0.1592 - val_loss: 0.1216 - val_mean_absolute_error: 0.1820\n",
      "Epoch 173/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0757 - mean_absolute_error: 0.1597 - val_loss: 0.1218 - val_mean_absolute_error: 0.1826\n",
      "Epoch 174/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0790 - mean_absolute_error: 0.1625 - val_loss: 0.1303 - val_mean_absolute_error: 0.1846\n",
      "Epoch 175/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0769 - mean_absolute_error: 0.1608 - val_loss: 0.1268 - val_mean_absolute_error: 0.1906\n",
      "Epoch 176/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0780 - mean_absolute_error: 0.1618 - val_loss: 0.1325 - val_mean_absolute_error: 0.1863\n",
      "Epoch 177/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0801 - mean_absolute_error: 0.1624 - val_loss: 0.1265 - val_mean_absolute_error: 0.1949\n",
      "Epoch 178/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0785 - mean_absolute_error: 0.1621 - val_loss: 0.1255 - val_mean_absolute_error: 0.1861\n",
      "Epoch 179/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0772 - mean_absolute_error: 0.1595 - val_loss: 0.1285 - val_mean_absolute_error: 0.1890\n",
      "Epoch 180/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0765 - mean_absolute_error: 0.1606 - val_loss: 0.1246 - val_mean_absolute_error: 0.1850\n",
      "Epoch 181/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0766 - mean_absolute_error: 0.1599 - val_loss: 0.1247 - val_mean_absolute_error: 0.1864\n",
      "Epoch 182/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0822 - mean_absolute_error: 0.1660 - val_loss: 0.1236 - val_mean_absolute_error: 0.1859\n",
      "Epoch 183/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0768 - mean_absolute_error: 0.1601 - val_loss: 0.1232 - val_mean_absolute_error: 0.1838\n",
      "Epoch 184/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0745 - mean_absolute_error: 0.1588 - val_loss: 0.1241 - val_mean_absolute_error: 0.1837\n",
      "Epoch 185/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0781 - mean_absolute_error: 0.1610 - val_loss: 0.1267 - val_mean_absolute_error: 0.1912\n",
      "Epoch 186/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0804 - mean_absolute_error: 0.1631 - val_loss: 0.1139 - val_mean_absolute_error: 0.1825\n",
      "Epoch 187/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0768 - mean_absolute_error: 0.1615 - val_loss: 0.1244 - val_mean_absolute_error: 0.1872\n",
      "Epoch 188/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0795 - mean_absolute_error: 0.1636 - val_loss: 0.1270 - val_mean_absolute_error: 0.1860\n",
      "Epoch 189/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0790 - mean_absolute_error: 0.1619 - val_loss: 0.1361 - val_mean_absolute_error: 0.1871\n",
      "Epoch 190/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0776 - mean_absolute_error: 0.1606 - val_loss: 0.1204 - val_mean_absolute_error: 0.1866\n",
      "Epoch 191/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0765 - mean_absolute_error: 0.1607 - val_loss: 0.1236 - val_mean_absolute_error: 0.1843\n",
      "Epoch 192/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0800 - mean_absolute_error: 0.1633 - val_loss: 0.1230 - val_mean_absolute_error: 0.1865\n",
      "Epoch 193/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.0753 - mean_absolute_error: 0.1591 - val_loss: 0.1227 - val_mean_absolute_error: 0.1839\n",
      "Epoch 194/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0782 - mean_absolute_error: 0.1604 - val_loss: 0.1280 - val_mean_absolute_error: 0.1872\n",
      "Epoch 195/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0849 - mean_absolute_error: 0.1660 - val_loss: 0.1974 - val_mean_absolute_error: 0.2051\n",
      "Epoch 196/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0856 - mean_absolute_error: 0.1671 - val_loss: 0.1215 - val_mean_absolute_error: 0.1874\n",
      "Epoch 197/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0750 - mean_absolute_error: 0.1590 - val_loss: 0.1197 - val_mean_absolute_error: 0.1848\n",
      "Epoch 198/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0756 - mean_absolute_error: 0.1590 - val_loss: 0.1330 - val_mean_absolute_error: 0.1909\n",
      "Epoch 199/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0782 - mean_absolute_error: 0.1613 - val_loss: 0.1288 - val_mean_absolute_error: 0.1867\n",
      "Epoch 200/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0858 - mean_absolute_error: 0.1686 - val_loss: 0.1240 - val_mean_absolute_error: 0.1876\n",
      "Epoch 201/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0779 - mean_absolute_error: 0.1611 - val_loss: 0.1243 - val_mean_absolute_error: 0.1916\n",
      "Epoch 202/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0752 - mean_absolute_error: 0.1603 - val_loss: 0.1225 - val_mean_absolute_error: 0.1853\n",
      "Epoch 203/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0726 - mean_absolute_error: 0.1565 - val_loss: 0.1220 - val_mean_absolute_error: 0.1830\n",
      "Epoch 204/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0780 - mean_absolute_error: 0.1632 - val_loss: 0.1309 - val_mean_absolute_error: 0.1914\n",
      "Epoch 205/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0755 - mean_absolute_error: 0.1588 - val_loss: 0.1239 - val_mean_absolute_error: 0.1874\n",
      "Epoch 206/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0747 - mean_absolute_error: 0.1587 - val_loss: 0.1227 - val_mean_absolute_error: 0.1841\n",
      "Epoch 207/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0740 - mean_absolute_error: 0.1580 - val_loss: 0.1293 - val_mean_absolute_error: 0.1877\n",
      "Epoch 208/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0780 - mean_absolute_error: 0.1613 - val_loss: 0.1313 - val_mean_absolute_error: 0.1918\n",
      "Epoch 209/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0780 - mean_absolute_error: 0.1607 - val_loss: 0.1273 - val_mean_absolute_error: 0.1851\n",
      "Epoch 210/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0773 - mean_absolute_error: 0.1617 - val_loss: 0.1249 - val_mean_absolute_error: 0.1918\n",
      "Epoch 211/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0776 - mean_absolute_error: 0.1616 - val_loss: 0.1213 - val_mean_absolute_error: 0.1840\n",
      "Epoch 212/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0790 - mean_absolute_error: 0.1624 - val_loss: 0.1220 - val_mean_absolute_error: 0.1851\n",
      "Epoch 213/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0750 - mean_absolute_error: 0.1592 - val_loss: 0.1285 - val_mean_absolute_error: 0.1907\n",
      "Epoch 214/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0752 - mean_absolute_error: 0.1590 - val_loss: 0.1247 - val_mean_absolute_error: 0.1885\n",
      "Epoch 215/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0748 - mean_absolute_error: 0.1592 - val_loss: 0.1243 - val_mean_absolute_error: 0.1861\n",
      "Epoch 216/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0771 - mean_absolute_error: 0.1604 - val_loss: 0.1279 - val_mean_absolute_error: 0.1863\n",
      "Epoch 217/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0761 - mean_absolute_error: 0.1594 - val_loss: 0.1333 - val_mean_absolute_error: 0.2051\n",
      "Epoch 218/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0759 - mean_absolute_error: 0.1592 - val_loss: 0.1297 - val_mean_absolute_error: 0.1945\n",
      "Epoch 219/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0770 - mean_absolute_error: 0.1622 - val_loss: 0.1301 - val_mean_absolute_error: 0.1910\n",
      "Epoch 220/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0753 - mean_absolute_error: 0.1593 - val_loss: 0.1319 - val_mean_absolute_error: 0.1897\n",
      "Epoch 221/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0757 - mean_absolute_error: 0.1592 - val_loss: 0.1224 - val_mean_absolute_error: 0.1874\n",
      "Epoch 222/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0806 - mean_absolute_error: 0.1632 - val_loss: 0.1304 - val_mean_absolute_error: 0.1920\n",
      "Epoch 223/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0829 - mean_absolute_error: 0.1641 - val_loss: 0.1337 - val_mean_absolute_error: 0.1915\n",
      "Epoch 224/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0772 - mean_absolute_error: 0.1614 - val_loss: 0.1229 - val_mean_absolute_error: 0.1873\n",
      "Epoch 225/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0763 - mean_absolute_error: 0.1599 - val_loss: 0.1287 - val_mean_absolute_error: 0.1927\n",
      "Epoch 226/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0765 - mean_absolute_error: 0.1610 - val_loss: 0.1213 - val_mean_absolute_error: 0.1856\n",
      "Epoch 227/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0772 - mean_absolute_error: 0.1615 - val_loss: 0.1172 - val_mean_absolute_error: 0.1823\n",
      "Epoch 228/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.0795 - mean_absolute_error: 0.1625 - val_loss: 0.1223 - val_mean_absolute_error: 0.1846\n",
      "Epoch 229/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0764 - mean_absolute_error: 0.1599 - val_loss: 0.1227 - val_mean_absolute_error: 0.1839\n",
      "Epoch 230/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0728 - mean_absolute_error: 0.1566 - val_loss: 0.1203 - val_mean_absolute_error: 0.1847\n",
      "Epoch 231/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0721 - mean_absolute_error: 0.1566 - val_loss: 0.1274 - val_mean_absolute_error: 0.1868\n",
      "Epoch 232/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0737 - mean_absolute_error: 0.1575 - val_loss: 0.1240 - val_mean_absolute_error: 0.1862\n",
      "Epoch 233/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0724 - mean_absolute_error: 0.1563 - val_loss: 0.1218 - val_mean_absolute_error: 0.1865\n",
      "Epoch 234/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0723 - mean_absolute_error: 0.1561 - val_loss: 0.1332 - val_mean_absolute_error: 0.1965\n",
      "Epoch 235/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0788 - mean_absolute_error: 0.1625 - val_loss: 0.1275 - val_mean_absolute_error: 0.1880\n",
      "Epoch 236/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0763 - mean_absolute_error: 0.1613 - val_loss: 0.1198 - val_mean_absolute_error: 0.1837\n",
      "Epoch 237/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0726 - mean_absolute_error: 0.1572 - val_loss: 0.1256 - val_mean_absolute_error: 0.1872\n",
      "Epoch 238/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0753 - mean_absolute_error: 0.1591 - val_loss: 0.1250 - val_mean_absolute_error: 0.1875\n",
      "Epoch 239/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0866 - mean_absolute_error: 0.1658 - val_loss: 0.1314 - val_mean_absolute_error: 0.1915\n",
      "Epoch 240/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0766 - mean_absolute_error: 0.1603 - val_loss: 0.1271 - val_mean_absolute_error: 0.1876\n",
      "Epoch 241/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0722 - mean_absolute_error: 0.1567 - val_loss: 0.1236 - val_mean_absolute_error: 0.1847\n",
      "Epoch 242/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0733 - mean_absolute_error: 0.1572 - val_loss: 0.1282 - val_mean_absolute_error: 0.1855\n",
      "Epoch 243/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0709 - mean_absolute_error: 0.1555 - val_loss: 0.1229 - val_mean_absolute_error: 0.1865\n",
      "Epoch 244/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0715 - mean_absolute_error: 0.1556 - val_loss: 0.1214 - val_mean_absolute_error: 0.1836\n",
      "Epoch 245/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0763 - mean_absolute_error: 0.1601 - val_loss: 0.1275 - val_mean_absolute_error: 0.1867\n",
      "Epoch 246/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0790 - mean_absolute_error: 0.1616 - val_loss: 0.1276 - val_mean_absolute_error: 0.1886\n",
      "Epoch 247/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0760 - mean_absolute_error: 0.1590 - val_loss: 0.1209 - val_mean_absolute_error: 0.1832\n",
      "Epoch 248/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0719 - mean_absolute_error: 0.1561 - val_loss: 0.1204 - val_mean_absolute_error: 0.1846\n",
      "Epoch 249/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0734 - mean_absolute_error: 0.1584 - val_loss: 0.1185 - val_mean_absolute_error: 0.1836\n",
      "Epoch 250/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0728 - mean_absolute_error: 0.1575 - val_loss: 0.1222 - val_mean_absolute_error: 0.1864\n",
      "Epoch 251/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0738 - mean_absolute_error: 0.1588 - val_loss: 0.1441 - val_mean_absolute_error: 0.1940\n",
      "Epoch 252/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0772 - mean_absolute_error: 0.1604 - val_loss: 0.1475 - val_mean_absolute_error: 0.1905\n",
      "Epoch 253/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0821 - mean_absolute_error: 0.1644 - val_loss: 0.1322 - val_mean_absolute_error: 0.1924\n",
      "Epoch 254/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0731 - mean_absolute_error: 0.1577 - val_loss: 0.1249 - val_mean_absolute_error: 0.1881\n",
      "Epoch 255/300\n",
      "12967/12967 [==============================] - 0s 32us/sample - loss: 0.0750 - mean_absolute_error: 0.1605 - val_loss: 0.1252 - val_mean_absolute_error: 0.1844\n",
      "Epoch 256/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0799 - mean_absolute_error: 0.1639 - val_loss: 0.1190 - val_mean_absolute_error: 0.1863\n",
      "Epoch 257/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0723 - mean_absolute_error: 0.1569 - val_loss: 0.1275 - val_mean_absolute_error: 0.1857\n",
      "Epoch 258/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0707 - mean_absolute_error: 0.1548 - val_loss: 0.1213 - val_mean_absolute_error: 0.1882\n",
      "Epoch 259/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0760 - mean_absolute_error: 0.1595 - val_loss: 0.1314 - val_mean_absolute_error: 0.1879\n",
      "Epoch 260/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0732 - mean_absolute_error: 0.1575 - val_loss: 0.1207 - val_mean_absolute_error: 0.1817\n",
      "Epoch 261/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0746 - mean_absolute_error: 0.1585 - val_loss: 0.1257 - val_mean_absolute_error: 0.1863\n",
      "Epoch 262/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0728 - mean_absolute_error: 0.1574 - val_loss: 0.1240 - val_mean_absolute_error: 0.1861\n",
      "Epoch 263/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0706 - mean_absolute_error: 0.1550 - val_loss: 0.1188 - val_mean_absolute_error: 0.1840\n",
      "Epoch 264/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0749 - mean_absolute_error: 0.1590 - val_loss: 0.1229 - val_mean_absolute_error: 0.1881\n",
      "Epoch 265/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0776 - mean_absolute_error: 0.1615 - val_loss: 0.1202 - val_mean_absolute_error: 0.1851\n",
      "Epoch 266/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0711 - mean_absolute_error: 0.1562 - val_loss: 0.1215 - val_mean_absolute_error: 0.1835\n",
      "Epoch 267/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0715 - mean_absolute_error: 0.1558 - val_loss: 0.1295 - val_mean_absolute_error: 0.1951\n",
      "Epoch 268/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0717 - mean_absolute_error: 0.1564 - val_loss: 0.1229 - val_mean_absolute_error: 0.1883\n",
      "Epoch 269/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0743 - mean_absolute_error: 0.1581 - val_loss: 0.1262 - val_mean_absolute_error: 0.1931\n",
      "Epoch 270/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0739 - mean_absolute_error: 0.1594 - val_loss: 0.1275 - val_mean_absolute_error: 0.1868\n",
      "Epoch 271/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0731 - mean_absolute_error: 0.1575 - val_loss: 0.1169 - val_mean_absolute_error: 0.1830\n",
      "Epoch 272/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0711 - mean_absolute_error: 0.1557 - val_loss: 0.1194 - val_mean_absolute_error: 0.1834\n",
      "Epoch 273/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0762 - mean_absolute_error: 0.1610 - val_loss: 0.1171 - val_mean_absolute_error: 0.1828\n",
      "Epoch 274/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0790 - mean_absolute_error: 0.1624 - val_loss: 0.1239 - val_mean_absolute_error: 0.1881\n",
      "Epoch 275/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0800 - mean_absolute_error: 0.1630 - val_loss: 0.1223 - val_mean_absolute_error: 0.1854\n",
      "Epoch 276/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0732 - mean_absolute_error: 0.1572 - val_loss: 0.1162 - val_mean_absolute_error: 0.1821\n",
      "Epoch 277/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0705 - mean_absolute_error: 0.1552 - val_loss: 0.1192 - val_mean_absolute_error: 0.1871\n",
      "Epoch 278/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0729 - mean_absolute_error: 0.1579 - val_loss: 0.1259 - val_mean_absolute_error: 0.1880\n",
      "Epoch 279/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0722 - mean_absolute_error: 0.1563 - val_loss: 0.1271 - val_mean_absolute_error: 0.1865\n",
      "Epoch 280/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0714 - mean_absolute_error: 0.1558 - val_loss: 0.1294 - val_mean_absolute_error: 0.1873\n",
      "Epoch 281/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0770 - mean_absolute_error: 0.1608 - val_loss: 0.1234 - val_mean_absolute_error: 0.1845\n",
      "Epoch 282/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0729 - mean_absolute_error: 0.1587 - val_loss: 0.1255 - val_mean_absolute_error: 0.1925\n",
      "Epoch 283/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0727 - mean_absolute_error: 0.1584 - val_loss: 0.1205 - val_mean_absolute_error: 0.1889\n",
      "Epoch 284/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0735 - mean_absolute_error: 0.1580 - val_loss: 0.1197 - val_mean_absolute_error: 0.1865\n",
      "Epoch 285/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0752 - mean_absolute_error: 0.1592 - val_loss: 0.1288 - val_mean_absolute_error: 0.1897\n",
      "Epoch 286/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0767 - mean_absolute_error: 0.1606 - val_loss: 0.1252 - val_mean_absolute_error: 0.1964\n",
      "Epoch 287/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0811 - mean_absolute_error: 0.1645 - val_loss: 0.1235 - val_mean_absolute_error: 0.1856\n",
      "Epoch 288/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0728 - mean_absolute_error: 0.1571 - val_loss: 0.1202 - val_mean_absolute_error: 0.1839\n",
      "Epoch 289/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0730 - mean_absolute_error: 0.1586 - val_loss: 0.1244 - val_mean_absolute_error: 0.1885\n",
      "Epoch 290/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0720 - mean_absolute_error: 0.1568 - val_loss: 0.1203 - val_mean_absolute_error: 0.1839\n",
      "Epoch 291/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0742 - mean_absolute_error: 0.1586 - val_loss: 0.1245 - val_mean_absolute_error: 0.1913\n",
      "Epoch 292/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0806 - mean_absolute_error: 0.1631 - val_loss: 0.1204 - val_mean_absolute_error: 0.1830\n",
      "Epoch 293/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0767 - mean_absolute_error: 0.1614 - val_loss: 0.1231 - val_mean_absolute_error: 0.1852\n",
      "Epoch 294/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0725 - mean_absolute_error: 0.1563 - val_loss: 0.1192 - val_mean_absolute_error: 0.1837\n",
      "Epoch 295/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0727 - mean_absolute_error: 0.1568 - val_loss: 0.1237 - val_mean_absolute_error: 0.1856\n",
      "Epoch 296/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0733 - mean_absolute_error: 0.1578 - val_loss: 0.1275 - val_mean_absolute_error: 0.1858\n",
      "Epoch 297/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0730 - mean_absolute_error: 0.1572 - val_loss: 0.1219 - val_mean_absolute_error: 0.1845\n",
      "Epoch 298/300\n",
      "12967/12967 [==============================] - 0s 31us/sample - loss: 0.0690 - mean_absolute_error: 0.1544 - val_loss: 0.1214 - val_mean_absolute_error: 0.1836\n",
      "Epoch 299/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0690 - mean_absolute_error: 0.1546 - val_loss: 0.1250 - val_mean_absolute_error: 0.1909\n",
      "Epoch 300/300\n",
      "12967/12967 [==============================] - 0s 30us/sample - loss: 0.0698 - mean_absolute_error: 0.1549 - val_loss: 0.1231 - val_mean_absolute_error: 0.1878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f45a8306f98>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = keras.Sequential(name='model-3')\n",
    "model_3.add(layers.Dense(64, \n",
    "                         kernel_regularizer=keras.regularizers.l2(0.001), \n",
    "                         activation='relu', input_shape=(21,)))\n",
    "model_3.add(layers.Dense(64, kernel_regularizer=keras.regularizers.l2(0.001), activation='relu'))\n",
    "model_3.add(layers.Dense(1))\n",
    "\n",
    "model_3.compile(keras.optimizers.Adam(0.001),\n",
    "                loss=keras.losses.MeanSquaredError(),\n",
    "                metrics=[keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "log_dir = os.path.join('lab2-logs', 'model-3')\n",
    "model_cbk = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "model_mckp = keras.callbacks.ModelCheckpoint(model_dir + '/Best-model-3.h5', \n",
    "                                             monitor='val_mean_absolute_error', \n",
    "                                             save_best_only=True, \n",
    "                                             mode='min')\n",
    "model_3.fit(x_train, y_train, \n",
    "            batch_size=64 ,\n",
    "            epochs=300, \n",
    "            validation_data=(x_val, y_val), \n",
    "            callbacks=[model_cbk, model_mckp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加入 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12967 samples, validate on 4323 samples\n",
      "Epoch 1/300\n",
      "12967/12967 [==============================] - 1s 59us/sample - loss: 0.5470 - mean_absolute_error: 0.4688 - val_loss: 0.2764 - val_mean_absolute_error: 0.3119\n",
      "Epoch 2/300\n",
      "12967/12967 [==============================] - 0s 37us/sample - loss: 0.3332 - mean_absolute_error: 0.3627 - val_loss: 0.2323 - val_mean_absolute_error: 0.2979\n",
      "Epoch 3/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.2863 - mean_absolute_error: 0.3357 - val_loss: 0.2092 - val_mean_absolute_error: 0.2846\n",
      "Epoch 4/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.2742 - mean_absolute_error: 0.3227 - val_loss: 0.2288 - val_mean_absolute_error: 0.2828\n",
      "Epoch 5/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.2432 - mean_absolute_error: 0.3073 - val_loss: 0.2353 - val_mean_absolute_error: 0.2853\n",
      "Epoch 6/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.2443 - mean_absolute_error: 0.3036 - val_loss: 0.2301 - val_mean_absolute_error: 0.2847\n",
      "Epoch 7/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.2160 - mean_absolute_error: 0.2934 - val_loss: 0.1892 - val_mean_absolute_error: 0.2635\n",
      "Epoch 8/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.2168 - mean_absolute_error: 0.2908 - val_loss: 0.1902 - val_mean_absolute_error: 0.2629\n",
      "Epoch 9/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.2095 - mean_absolute_error: 0.2852 - val_loss: 0.1679 - val_mean_absolute_error: 0.2501\n",
      "Epoch 10/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.2090 - mean_absolute_error: 0.2802 - val_loss: 0.1768 - val_mean_absolute_error: 0.2563\n",
      "Epoch 11/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1996 - mean_absolute_error: 0.2784 - val_loss: 0.1644 - val_mean_absolute_error: 0.2494\n",
      "Epoch 12/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1943 - mean_absolute_error: 0.2731 - val_loss: 0.1764 - val_mean_absolute_error: 0.2537\n",
      "Epoch 13/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1880 - mean_absolute_error: 0.2707 - val_loss: 0.1529 - val_mean_absolute_error: 0.2410\n",
      "Epoch 14/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1874 - mean_absolute_error: 0.2684 - val_loss: 0.1541 - val_mean_absolute_error: 0.2372\n",
      "Epoch 15/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1784 - mean_absolute_error: 0.2615 - val_loss: 0.1459 - val_mean_absolute_error: 0.2384\n",
      "Epoch 16/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1781 - mean_absolute_error: 0.2631 - val_loss: 0.1462 - val_mean_absolute_error: 0.2468\n",
      "Epoch 17/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1843 - mean_absolute_error: 0.2618 - val_loss: 0.1536 - val_mean_absolute_error: 0.2407\n",
      "Epoch 18/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1763 - mean_absolute_error: 0.2594 - val_loss: 0.1813 - val_mean_absolute_error: 0.2571\n",
      "Epoch 19/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1755 - mean_absolute_error: 0.2588 - val_loss: 0.1526 - val_mean_absolute_error: 0.2392\n",
      "Epoch 20/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1690 - mean_absolute_error: 0.2548 - val_loss: 0.1402 - val_mean_absolute_error: 0.2438\n",
      "Epoch 21/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1624 - mean_absolute_error: 0.2561 - val_loss: 0.1480 - val_mean_absolute_error: 0.2413\n",
      "Epoch 22/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1607 - mean_absolute_error: 0.2531 - val_loss: 0.1469 - val_mean_absolute_error: 0.2336\n",
      "Epoch 23/300\n",
      "12967/12967 [==============================] - 0s 37us/sample - loss: 0.1593 - mean_absolute_error: 0.2519 - val_loss: 0.1353 - val_mean_absolute_error: 0.2325\n",
      "Epoch 24/300\n",
      "12967/12967 [==============================] - 0s 37us/sample - loss: 0.1577 - mean_absolute_error: 0.2506 - val_loss: 0.1327 - val_mean_absolute_error: 0.2250\n",
      "Epoch 25/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1699 - mean_absolute_error: 0.2511 - val_loss: 0.1384 - val_mean_absolute_error: 0.2243\n",
      "Epoch 26/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1474 - mean_absolute_error: 0.2440 - val_loss: 0.1567 - val_mean_absolute_error: 0.2275\n",
      "Epoch 27/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1546 - mean_absolute_error: 0.2472 - val_loss: 0.1340 - val_mean_absolute_error: 0.2256\n",
      "Epoch 28/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1524 - mean_absolute_error: 0.2461 - val_loss: 0.1400 - val_mean_absolute_error: 0.2277\n",
      "Epoch 29/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1584 - mean_absolute_error: 0.2448 - val_loss: 0.1645 - val_mean_absolute_error: 0.2360\n",
      "Epoch 30/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1530 - mean_absolute_error: 0.2466 - val_loss: 0.1426 - val_mean_absolute_error: 0.2262\n",
      "Epoch 31/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1451 - mean_absolute_error: 0.2423 - val_loss: 0.1483 - val_mean_absolute_error: 0.2274\n",
      "Epoch 32/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1472 - mean_absolute_error: 0.2419 - val_loss: 0.1368 - val_mean_absolute_error: 0.2275\n",
      "Epoch 33/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1576 - mean_absolute_error: 0.2453 - val_loss: 0.1418 - val_mean_absolute_error: 0.2294\n",
      "Epoch 34/300\n",
      "12967/12967 [==============================] - 0s 37us/sample - loss: 0.1510 - mean_absolute_error: 0.2407 - val_loss: 0.1285 - val_mean_absolute_error: 0.2123\n",
      "Epoch 35/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1500 - mean_absolute_error: 0.2411 - val_loss: 0.1282 - val_mean_absolute_error: 0.2220\n",
      "Epoch 36/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1436 - mean_absolute_error: 0.2377 - val_loss: 0.1397 - val_mean_absolute_error: 0.2197\n",
      "Epoch 37/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1443 - mean_absolute_error: 0.2387 - val_loss: 0.1463 - val_mean_absolute_error: 0.2240\n",
      "Epoch 38/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1498 - mean_absolute_error: 0.2389 - val_loss: 0.1276 - val_mean_absolute_error: 0.2177\n",
      "Epoch 39/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1480 - mean_absolute_error: 0.2396 - val_loss: 0.1357 - val_mean_absolute_error: 0.2203\n",
      "Epoch 40/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1384 - mean_absolute_error: 0.2344 - val_loss: 0.1452 - val_mean_absolute_error: 0.2150\n",
      "Epoch 41/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1490 - mean_absolute_error: 0.2381 - val_loss: 0.1404 - val_mean_absolute_error: 0.2185\n",
      "Epoch 42/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1396 - mean_absolute_error: 0.2351 - val_loss: 0.1230 - val_mean_absolute_error: 0.2174\n",
      "Epoch 43/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1503 - mean_absolute_error: 0.2368 - val_loss: 0.1273 - val_mean_absolute_error: 0.2087\n",
      "Epoch 44/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1409 - mean_absolute_error: 0.2344 - val_loss: 0.1362 - val_mean_absolute_error: 0.2244\n",
      "Epoch 45/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1412 - mean_absolute_error: 0.2337 - val_loss: 0.1216 - val_mean_absolute_error: 0.2140\n",
      "Epoch 46/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1422 - mean_absolute_error: 0.2349 - val_loss: 0.1162 - val_mean_absolute_error: 0.2137\n",
      "Epoch 47/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1336 - mean_absolute_error: 0.2308 - val_loss: 0.1201 - val_mean_absolute_error: 0.2152\n",
      "Epoch 48/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1348 - mean_absolute_error: 0.2319 - val_loss: 0.1344 - val_mean_absolute_error: 0.2171\n",
      "Epoch 49/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1363 - mean_absolute_error: 0.2305 - val_loss: 0.1596 - val_mean_absolute_error: 0.2314\n",
      "Epoch 50/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1320 - mean_absolute_error: 0.2291 - val_loss: 0.1170 - val_mean_absolute_error: 0.2072\n",
      "Epoch 51/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1381 - mean_absolute_error: 0.2321 - val_loss: 0.1272 - val_mean_absolute_error: 0.2185\n",
      "Epoch 52/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1321 - mean_absolute_error: 0.2295 - val_loss: 0.1361 - val_mean_absolute_error: 0.2135\n",
      "Epoch 53/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1548 - mean_absolute_error: 0.2363 - val_loss: 0.1279 - val_mean_absolute_error: 0.2134\n",
      "Epoch 54/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1399 - mean_absolute_error: 0.2316 - val_loss: 0.1282 - val_mean_absolute_error: 0.2080\n",
      "Epoch 55/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1380 - mean_absolute_error: 0.2302 - val_loss: 0.1276 - val_mean_absolute_error: 0.2152\n",
      "Epoch 56/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1339 - mean_absolute_error: 0.2285 - val_loss: 0.1294 - val_mean_absolute_error: 0.2153\n",
      "Epoch 57/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1391 - mean_absolute_error: 0.2295 - val_loss: 0.1275 - val_mean_absolute_error: 0.2164\n",
      "Epoch 58/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1405 - mean_absolute_error: 0.2293 - val_loss: 0.1137 - val_mean_absolute_error: 0.2070\n",
      "Epoch 59/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1312 - mean_absolute_error: 0.2289 - val_loss: 0.1651 - val_mean_absolute_error: 0.2329\n",
      "Epoch 60/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1291 - mean_absolute_error: 0.2263 - val_loss: 0.1172 - val_mean_absolute_error: 0.2117\n",
      "Epoch 61/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1339 - mean_absolute_error: 0.2283 - val_loss: 0.1107 - val_mean_absolute_error: 0.2011\n",
      "Epoch 62/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1325 - mean_absolute_error: 0.2273 - val_loss: 0.1126 - val_mean_absolute_error: 0.2095\n",
      "Epoch 63/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1309 - mean_absolute_error: 0.2280 - val_loss: 0.1214 - val_mean_absolute_error: 0.2113\n",
      "Epoch 64/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1295 - mean_absolute_error: 0.2268 - val_loss: 0.1125 - val_mean_absolute_error: 0.2100\n",
      "Epoch 65/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1326 - mean_absolute_error: 0.2282 - val_loss: 0.1227 - val_mean_absolute_error: 0.2136\n",
      "Epoch 66/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1329 - mean_absolute_error: 0.2298 - val_loss: 0.1187 - val_mean_absolute_error: 0.2101\n",
      "Epoch 67/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1269 - mean_absolute_error: 0.2258 - val_loss: 0.1765 - val_mean_absolute_error: 0.2379\n",
      "Epoch 68/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1312 - mean_absolute_error: 0.2273 - val_loss: 0.1299 - val_mean_absolute_error: 0.2207\n",
      "Epoch 69/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1281 - mean_absolute_error: 0.2253 - val_loss: 0.1174 - val_mean_absolute_error: 0.2200\n",
      "Epoch 70/300\n",
      "12967/12967 [==============================] - 0s 37us/sample - loss: 0.1293 - mean_absolute_error: 0.2253 - val_loss: 0.1122 - val_mean_absolute_error: 0.2121\n",
      "Epoch 71/300\n",
      "12967/12967 [==============================] - 1s 41us/sample - loss: 0.1192 - mean_absolute_error: 0.2226 - val_loss: 0.1158 - val_mean_absolute_error: 0.2178\n",
      "Epoch 72/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1307 - mean_absolute_error: 0.2247 - val_loss: 0.1251 - val_mean_absolute_error: 0.2181\n",
      "Epoch 73/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1283 - mean_absolute_error: 0.2246 - val_loss: 0.1212 - val_mean_absolute_error: 0.2151\n",
      "Epoch 74/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1249 - mean_absolute_error: 0.2240 - val_loss: 0.1113 - val_mean_absolute_error: 0.2054\n",
      "Epoch 75/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1282 - mean_absolute_error: 0.2248 - val_loss: 0.1166 - val_mean_absolute_error: 0.2121\n",
      "Epoch 76/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1217 - mean_absolute_error: 0.2217 - val_loss: 0.1212 - val_mean_absolute_error: 0.2094\n",
      "Epoch 77/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1249 - mean_absolute_error: 0.2216 - val_loss: 0.1371 - val_mean_absolute_error: 0.2183\n",
      "Epoch 78/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1246 - mean_absolute_error: 0.2235 - val_loss: 0.1160 - val_mean_absolute_error: 0.2164\n",
      "Epoch 79/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1249 - mean_absolute_error: 0.2246 - val_loss: 0.1192 - val_mean_absolute_error: 0.2066\n",
      "Epoch 80/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1317 - mean_absolute_error: 0.2263 - val_loss: 0.1406 - val_mean_absolute_error: 0.2168\n",
      "Epoch 81/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1228 - mean_absolute_error: 0.2245 - val_loss: 0.1153 - val_mean_absolute_error: 0.2162\n",
      "Epoch 82/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1254 - mean_absolute_error: 0.2227 - val_loss: 0.1167 - val_mean_absolute_error: 0.2085\n",
      "Epoch 83/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1144 - mean_absolute_error: 0.2220 - val_loss: 0.1110 - val_mean_absolute_error: 0.2094\n",
      "Epoch 84/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1185 - mean_absolute_error: 0.2213 - val_loss: 0.1145 - val_mean_absolute_error: 0.2199\n",
      "Epoch 85/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1240 - mean_absolute_error: 0.2219 - val_loss: 0.1305 - val_mean_absolute_error: 0.2153\n",
      "Epoch 86/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1194 - mean_absolute_error: 0.2199 - val_loss: 0.1474 - val_mean_absolute_error: 0.2215\n",
      "Epoch 87/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1191 - mean_absolute_error: 0.2227 - val_loss: 0.1159 - val_mean_absolute_error: 0.2100\n",
      "Epoch 88/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1248 - mean_absolute_error: 0.2227 - val_loss: 0.1308 - val_mean_absolute_error: 0.2271\n",
      "Epoch 89/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1220 - mean_absolute_error: 0.2221 - val_loss: 0.1248 - val_mean_absolute_error: 0.2205\n",
      "Epoch 90/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1184 - mean_absolute_error: 0.2187 - val_loss: 0.1311 - val_mean_absolute_error: 0.2140\n",
      "Epoch 91/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1166 - mean_absolute_error: 0.2201 - val_loss: 0.1192 - val_mean_absolute_error: 0.2130\n",
      "Epoch 92/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1199 - mean_absolute_error: 0.2205 - val_loss: 0.1139 - val_mean_absolute_error: 0.2128\n",
      "Epoch 93/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1216 - mean_absolute_error: 0.2198 - val_loss: 0.1287 - val_mean_absolute_error: 0.2262\n",
      "Epoch 94/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1274 - mean_absolute_error: 0.2216 - val_loss: 0.1208 - val_mean_absolute_error: 0.2148\n",
      "Epoch 95/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1160 - mean_absolute_error: 0.2192 - val_loss: 0.1170 - val_mean_absolute_error: 0.2103\n",
      "Epoch 96/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1130 - mean_absolute_error: 0.2183 - val_loss: 0.1146 - val_mean_absolute_error: 0.2117\n",
      "Epoch 97/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1210 - mean_absolute_error: 0.2198 - val_loss: 0.1224 - val_mean_absolute_error: 0.2147\n",
      "Epoch 98/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1275 - mean_absolute_error: 0.2227 - val_loss: 0.1078 - val_mean_absolute_error: 0.2062\n",
      "Epoch 99/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1203 - mean_absolute_error: 0.2210 - val_loss: 0.1189 - val_mean_absolute_error: 0.2172\n",
      "Epoch 100/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1196 - mean_absolute_error: 0.2177 - val_loss: 0.1140 - val_mean_absolute_error: 0.2199\n",
      "Epoch 101/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1209 - mean_absolute_error: 0.2197 - val_loss: 0.1045 - val_mean_absolute_error: 0.1964\n",
      "Epoch 102/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1167 - mean_absolute_error: 0.2197 - val_loss: 0.1228 - val_mean_absolute_error: 0.2140\n",
      "Epoch 103/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1203 - mean_absolute_error: 0.2200 - val_loss: 0.1184 - val_mean_absolute_error: 0.2189\n",
      "Epoch 104/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1189 - mean_absolute_error: 0.2191 - val_loss: 0.1089 - val_mean_absolute_error: 0.1924\n",
      "Epoch 105/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1182 - mean_absolute_error: 0.2189 - val_loss: 0.1474 - val_mean_absolute_error: 0.2236\n",
      "Epoch 106/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1168 - mean_absolute_error: 0.2198 - val_loss: 0.1131 - val_mean_absolute_error: 0.2161\n",
      "Epoch 107/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1232 - mean_absolute_error: 0.2198 - val_loss: 0.1109 - val_mean_absolute_error: 0.1991\n",
      "Epoch 108/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1209 - mean_absolute_error: 0.2188 - val_loss: 0.1189 - val_mean_absolute_error: 0.2087\n",
      "Epoch 109/300\n",
      "12967/12967 [==============================] - 0s 37us/sample - loss: 0.1200 - mean_absolute_error: 0.2196 - val_loss: 0.1266 - val_mean_absolute_error: 0.2101\n",
      "Epoch 110/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1171 - mean_absolute_error: 0.2192 - val_loss: 0.1087 - val_mean_absolute_error: 0.2013\n",
      "Epoch 111/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1204 - mean_absolute_error: 0.2211 - val_loss: 0.1134 - val_mean_absolute_error: 0.2080\n",
      "Epoch 112/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1162 - mean_absolute_error: 0.2186 - val_loss: 0.1304 - val_mean_absolute_error: 0.2154\n",
      "Epoch 113/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1208 - mean_absolute_error: 0.2188 - val_loss: 0.1160 - val_mean_absolute_error: 0.2080\n",
      "Epoch 114/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1187 - mean_absolute_error: 0.2177 - val_loss: 0.1085 - val_mean_absolute_error: 0.2104\n",
      "Epoch 115/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1117 - mean_absolute_error: 0.2191 - val_loss: 0.1223 - val_mean_absolute_error: 0.2043\n",
      "Epoch 116/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1160 - mean_absolute_error: 0.2180 - val_loss: 0.1284 - val_mean_absolute_error: 0.2214\n",
      "Epoch 117/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1208 - mean_absolute_error: 0.2193 - val_loss: 0.1370 - val_mean_absolute_error: 0.2080\n",
      "Epoch 118/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1185 - mean_absolute_error: 0.2191 - val_loss: 0.1348 - val_mean_absolute_error: 0.2033\n",
      "Epoch 119/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1192 - mean_absolute_error: 0.2204 - val_loss: 0.1142 - val_mean_absolute_error: 0.2097\n",
      "Epoch 120/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1164 - mean_absolute_error: 0.2191 - val_loss: 0.1160 - val_mean_absolute_error: 0.2165\n",
      "Epoch 121/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1199 - mean_absolute_error: 0.2189 - val_loss: 0.1088 - val_mean_absolute_error: 0.2120\n",
      "Epoch 122/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1185 - mean_absolute_error: 0.2162 - val_loss: 0.1285 - val_mean_absolute_error: 0.2205\n",
      "Epoch 123/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1063 - mean_absolute_error: 0.2141 - val_loss: 0.1169 - val_mean_absolute_error: 0.2159\n",
      "Epoch 124/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1088 - mean_absolute_error: 0.2145 - val_loss: 0.1181 - val_mean_absolute_error: 0.2032\n",
      "Epoch 125/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1147 - mean_absolute_error: 0.2176 - val_loss: 0.1171 - val_mean_absolute_error: 0.2056\n",
      "Epoch 126/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1124 - mean_absolute_error: 0.2174 - val_loss: 0.1144 - val_mean_absolute_error: 0.2043\n",
      "Epoch 127/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1142 - mean_absolute_error: 0.2173 - val_loss: 0.1177 - val_mean_absolute_error: 0.2128\n",
      "Epoch 128/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1107 - mean_absolute_error: 0.2156 - val_loss: 0.1159 - val_mean_absolute_error: 0.2113\n",
      "Epoch 129/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1236 - mean_absolute_error: 0.2194 - val_loss: 0.1121 - val_mean_absolute_error: 0.2135\n",
      "Epoch 130/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1085 - mean_absolute_error: 0.2134 - val_loss: 0.1462 - val_mean_absolute_error: 0.2392\n",
      "Epoch 131/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1115 - mean_absolute_error: 0.2156 - val_loss: 0.1263 - val_mean_absolute_error: 0.2168\n",
      "Epoch 132/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1177 - mean_absolute_error: 0.2177 - val_loss: 0.1143 - val_mean_absolute_error: 0.2075\n",
      "Epoch 133/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1137 - mean_absolute_error: 0.2191 - val_loss: 0.1182 - val_mean_absolute_error: 0.2017\n",
      "Epoch 134/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1197 - mean_absolute_error: 0.2182 - val_loss: 0.1087 - val_mean_absolute_error: 0.2064\n",
      "Epoch 135/300\n",
      "12967/12967 [==============================] - 0s 37us/sample - loss: 0.1157 - mean_absolute_error: 0.2185 - val_loss: 0.1160 - val_mean_absolute_error: 0.2068\n",
      "Epoch 136/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1103 - mean_absolute_error: 0.2164 - val_loss: 0.1317 - val_mean_absolute_error: 0.2150\n",
      "Epoch 137/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1168 - mean_absolute_error: 0.2175 - val_loss: 0.1148 - val_mean_absolute_error: 0.2045\n",
      "Epoch 138/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1141 - mean_absolute_error: 0.2175 - val_loss: 0.1128 - val_mean_absolute_error: 0.2097\n",
      "Epoch 139/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1227 - mean_absolute_error: 0.2191 - val_loss: 0.1268 - val_mean_absolute_error: 0.2107\n",
      "Epoch 140/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1155 - mean_absolute_error: 0.2181 - val_loss: 0.1254 - val_mean_absolute_error: 0.2202\n",
      "Epoch 141/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1228 - mean_absolute_error: 0.2204 - val_loss: 0.1101 - val_mean_absolute_error: 0.2070\n",
      "Epoch 142/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1201 - mean_absolute_error: 0.2200 - val_loss: 0.1268 - val_mean_absolute_error: 0.2183\n",
      "Epoch 143/300\n",
      "12967/12967 [==============================] - 0s 37us/sample - loss: 0.1125 - mean_absolute_error: 0.2179 - val_loss: 0.1388 - val_mean_absolute_error: 0.2216\n",
      "Epoch 144/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1265 - mean_absolute_error: 0.2199 - val_loss: 0.1928 - val_mean_absolute_error: 0.2370\n",
      "Epoch 145/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1152 - mean_absolute_error: 0.2181 - val_loss: 0.1187 - val_mean_absolute_error: 0.2059\n",
      "Epoch 146/300\n",
      "12967/12967 [==============================] - 0s 38us/sample - loss: 0.1217 - mean_absolute_error: 0.2175 - val_loss: 0.1221 - val_mean_absolute_error: 0.2203\n",
      "Epoch 147/300\n",
      "12967/12967 [==============================] - 1s 48us/sample - loss: 0.1187 - mean_absolute_error: 0.2207 - val_loss: 0.1383 - val_mean_absolute_error: 0.2316\n",
      "Epoch 148/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1126 - mean_absolute_error: 0.2149 - val_loss: 0.1275 - val_mean_absolute_error: 0.2144\n",
      "Epoch 149/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1266 - mean_absolute_error: 0.2191 - val_loss: 0.1179 - val_mean_absolute_error: 0.2160\n",
      "Epoch 150/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1111 - mean_absolute_error: 0.2172 - val_loss: 0.1175 - val_mean_absolute_error: 0.2112\n",
      "Epoch 151/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1118 - mean_absolute_error: 0.2159 - val_loss: 0.1237 - val_mean_absolute_error: 0.2146\n",
      "Epoch 152/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1261 - mean_absolute_error: 0.2191 - val_loss: 0.1402 - val_mean_absolute_error: 0.2172\n",
      "Epoch 153/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1117 - mean_absolute_error: 0.2160 - val_loss: 0.1312 - val_mean_absolute_error: 0.2150\n",
      "Epoch 154/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1078 - mean_absolute_error: 0.2146 - val_loss: 0.1204 - val_mean_absolute_error: 0.2101\n",
      "Epoch 155/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1066 - mean_absolute_error: 0.2146 - val_loss: 0.1242 - val_mean_absolute_error: 0.2019\n",
      "Epoch 156/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1129 - mean_absolute_error: 0.2152 - val_loss: 0.1290 - val_mean_absolute_error: 0.2093\n",
      "Epoch 157/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1142 - mean_absolute_error: 0.2164 - val_loss: 0.1339 - val_mean_absolute_error: 0.2187\n",
      "Epoch 158/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1143 - mean_absolute_error: 0.2163 - val_loss: 0.1239 - val_mean_absolute_error: 0.2222\n",
      "Epoch 159/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1115 - mean_absolute_error: 0.2156 - val_loss: 0.1132 - val_mean_absolute_error: 0.2074\n",
      "Epoch 160/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1105 - mean_absolute_error: 0.2153 - val_loss: 0.1394 - val_mean_absolute_error: 0.2125\n",
      "Epoch 161/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1194 - mean_absolute_error: 0.2177 - val_loss: 0.1304 - val_mean_absolute_error: 0.2170\n",
      "Epoch 162/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1090 - mean_absolute_error: 0.2150 - val_loss: 0.1193 - val_mean_absolute_error: 0.2167\n",
      "Epoch 163/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1154 - mean_absolute_error: 0.2170 - val_loss: 0.1084 - val_mean_absolute_error: 0.2004\n",
      "Epoch 164/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1141 - mean_absolute_error: 0.2176 - val_loss: 0.1194 - val_mean_absolute_error: 0.2106\n",
      "Epoch 165/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1163 - mean_absolute_error: 0.2150 - val_loss: 0.1112 - val_mean_absolute_error: 0.2065\n",
      "Epoch 166/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1227 - mean_absolute_error: 0.2173 - val_loss: 0.1251 - val_mean_absolute_error: 0.2137\n",
      "Epoch 167/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1114 - mean_absolute_error: 0.2156 - val_loss: 0.1140 - val_mean_absolute_error: 0.2054\n",
      "Epoch 168/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1063 - mean_absolute_error: 0.2141 - val_loss: 0.1251 - val_mean_absolute_error: 0.2066\n",
      "Epoch 169/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1147 - mean_absolute_error: 0.2142 - val_loss: 0.1660 - val_mean_absolute_error: 0.2407\n",
      "Epoch 170/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1205 - mean_absolute_error: 0.2196 - val_loss: 0.1276 - val_mean_absolute_error: 0.2130\n",
      "Epoch 171/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1093 - mean_absolute_error: 0.2166 - val_loss: 0.1119 - val_mean_absolute_error: 0.2166\n",
      "Epoch 172/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1190 - mean_absolute_error: 0.2166 - val_loss: 0.1124 - val_mean_absolute_error: 0.2133\n",
      "Epoch 173/300\n",
      "12967/12967 [==============================] - 0s 39us/sample - loss: 0.1144 - mean_absolute_error: 0.2162 - val_loss: 0.1239 - val_mean_absolute_error: 0.2089\n",
      "Epoch 174/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1111 - mean_absolute_error: 0.2157 - val_loss: 0.1056 - val_mean_absolute_error: 0.2078\n",
      "Epoch 175/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1151 - mean_absolute_error: 0.2167 - val_loss: 0.1246 - val_mean_absolute_error: 0.2187\n",
      "Epoch 176/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1125 - mean_absolute_error: 0.2149 - val_loss: 0.1246 - val_mean_absolute_error: 0.2270\n",
      "Epoch 177/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1094 - mean_absolute_error: 0.2150 - val_loss: 0.1293 - val_mean_absolute_error: 0.2163\n",
      "Epoch 178/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1134 - mean_absolute_error: 0.2149 - val_loss: 0.1199 - val_mean_absolute_error: 0.2228\n",
      "Epoch 179/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1073 - mean_absolute_error: 0.2140 - val_loss: 0.1159 - val_mean_absolute_error: 0.2093\n",
      "Epoch 180/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1045 - mean_absolute_error: 0.2120 - val_loss: 0.1145 - val_mean_absolute_error: 0.2064\n",
      "Epoch 181/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1139 - mean_absolute_error: 0.2150 - val_loss: 0.1118 - val_mean_absolute_error: 0.2010\n",
      "Epoch 182/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1112 - mean_absolute_error: 0.2138 - val_loss: 0.1174 - val_mean_absolute_error: 0.2211\n",
      "Epoch 183/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1143 - mean_absolute_error: 0.2153 - val_loss: 0.1333 - val_mean_absolute_error: 0.2153\n",
      "Epoch 184/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1148 - mean_absolute_error: 0.2162 - val_loss: 0.1167 - val_mean_absolute_error: 0.2219\n",
      "Epoch 185/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1130 - mean_absolute_error: 0.2160 - val_loss: 0.1194 - val_mean_absolute_error: 0.2117\n",
      "Epoch 186/300\n",
      "12967/12967 [==============================] - 0s 37us/sample - loss: 0.1111 - mean_absolute_error: 0.2146 - val_loss: 0.1146 - val_mean_absolute_error: 0.2025\n",
      "Epoch 187/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1153 - mean_absolute_error: 0.2168 - val_loss: 0.1229 - val_mean_absolute_error: 0.2136\n",
      "Epoch 188/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1055 - mean_absolute_error: 0.2132 - val_loss: 0.1146 - val_mean_absolute_error: 0.2174\n",
      "Epoch 189/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1136 - mean_absolute_error: 0.2145 - val_loss: 0.1387 - val_mean_absolute_error: 0.2232\n",
      "Epoch 190/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1145 - mean_absolute_error: 0.2142 - val_loss: 0.1301 - val_mean_absolute_error: 0.2260\n",
      "Epoch 191/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1134 - mean_absolute_error: 0.2150 - val_loss: 0.1180 - val_mean_absolute_error: 0.2065\n",
      "Epoch 192/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1148 - mean_absolute_error: 0.2146 - val_loss: 0.1556 - val_mean_absolute_error: 0.2340\n",
      "Epoch 193/300\n",
      "12967/12967 [==============================] - ETA: 0s - loss: 0.1094 - mean_absolute_error: 0.215 - 0s 35us/sample - loss: 0.1099 - mean_absolute_error: 0.2157 - val_loss: 0.1284 - val_mean_absolute_error: 0.2107\n",
      "Epoch 194/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1188 - mean_absolute_error: 0.2175 - val_loss: 0.1134 - val_mean_absolute_error: 0.1995\n",
      "Epoch 195/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1107 - mean_absolute_error: 0.2156 - val_loss: 0.1402 - val_mean_absolute_error: 0.2243\n",
      "Epoch 196/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1055 - mean_absolute_error: 0.2127 - val_loss: 0.1140 - val_mean_absolute_error: 0.2109\n",
      "Epoch 197/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1082 - mean_absolute_error: 0.2118 - val_loss: 0.1147 - val_mean_absolute_error: 0.2012\n",
      "Epoch 198/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1064 - mean_absolute_error: 0.2146 - val_loss: 0.1141 - val_mean_absolute_error: 0.2045\n",
      "Epoch 199/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1029 - mean_absolute_error: 0.2098 - val_loss: 0.1115 - val_mean_absolute_error: 0.2110\n",
      "Epoch 200/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1123 - mean_absolute_error: 0.2137 - val_loss: 0.1234 - val_mean_absolute_error: 0.2195\n",
      "Epoch 201/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1189 - mean_absolute_error: 0.2157 - val_loss: 0.1086 - val_mean_absolute_error: 0.2088\n",
      "Epoch 202/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1123 - mean_absolute_error: 0.2145 - val_loss: 0.1182 - val_mean_absolute_error: 0.2086\n",
      "Epoch 203/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1101 - mean_absolute_error: 0.2139 - val_loss: 0.1216 - val_mean_absolute_error: 0.2165\n",
      "Epoch 204/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1087 - mean_absolute_error: 0.2130 - val_loss: 0.1083 - val_mean_absolute_error: 0.2046\n",
      "Epoch 205/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1057 - mean_absolute_error: 0.2123 - val_loss: 0.1075 - val_mean_absolute_error: 0.2107\n",
      "Epoch 206/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1143 - mean_absolute_error: 0.2147 - val_loss: 0.1314 - val_mean_absolute_error: 0.2178\n",
      "Epoch 207/300\n",
      "12967/12967 [==============================] - 0s 37us/sample - loss: 0.1083 - mean_absolute_error: 0.2137 - val_loss: 0.1278 - val_mean_absolute_error: 0.2303\n",
      "Epoch 208/300\n",
      "12967/12967 [==============================] - 0s 38us/sample - loss: 0.1062 - mean_absolute_error: 0.2119 - val_loss: 0.1228 - val_mean_absolute_error: 0.2081\n",
      "Epoch 209/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1156 - mean_absolute_error: 0.2144 - val_loss: 0.1214 - val_mean_absolute_error: 0.2137\n",
      "Epoch 210/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1129 - mean_absolute_error: 0.2154 - val_loss: 0.1152 - val_mean_absolute_error: 0.2144\n",
      "Epoch 211/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1081 - mean_absolute_error: 0.2138 - val_loss: 0.1220 - val_mean_absolute_error: 0.2109\n",
      "Epoch 212/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1143 - mean_absolute_error: 0.2158 - val_loss: 0.1262 - val_mean_absolute_error: 0.2177\n",
      "Epoch 213/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1070 - mean_absolute_error: 0.2127 - val_loss: 0.1336 - val_mean_absolute_error: 0.2277\n",
      "Epoch 214/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1081 - mean_absolute_error: 0.2132 - val_loss: 0.1270 - val_mean_absolute_error: 0.2136\n",
      "Epoch 215/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1078 - mean_absolute_error: 0.2131 - val_loss: 0.1136 - val_mean_absolute_error: 0.2144\n",
      "Epoch 216/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1080 - mean_absolute_error: 0.2132 - val_loss: 0.1169 - val_mean_absolute_error: 0.2130\n",
      "Epoch 217/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1047 - mean_absolute_error: 0.2120 - val_loss: 0.1223 - val_mean_absolute_error: 0.2136\n",
      "Epoch 218/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1078 - mean_absolute_error: 0.2128 - val_loss: 0.1011 - val_mean_absolute_error: 0.2003\n",
      "Epoch 219/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1032 - mean_absolute_error: 0.2102 - val_loss: 0.1193 - val_mean_absolute_error: 0.2167\n",
      "Epoch 220/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1187 - mean_absolute_error: 0.2144 - val_loss: 0.1188 - val_mean_absolute_error: 0.2160\n",
      "Epoch 221/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1072 - mean_absolute_error: 0.2137 - val_loss: 0.1130 - val_mean_absolute_error: 0.2075\n",
      "Epoch 222/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1182 - mean_absolute_error: 0.2152 - val_loss: 0.1078 - val_mean_absolute_error: 0.2143\n",
      "Epoch 223/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1115 - mean_absolute_error: 0.2136 - val_loss: 0.1194 - val_mean_absolute_error: 0.2099\n",
      "Epoch 224/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1058 - mean_absolute_error: 0.2126 - val_loss: 0.1183 - val_mean_absolute_error: 0.2250\n",
      "Epoch 225/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1069 - mean_absolute_error: 0.2113 - val_loss: 0.1121 - val_mean_absolute_error: 0.2277\n",
      "Epoch 226/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1139 - mean_absolute_error: 0.2153 - val_loss: 0.1194 - val_mean_absolute_error: 0.2261\n",
      "Epoch 227/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1049 - mean_absolute_error: 0.2111 - val_loss: 0.1093 - val_mean_absolute_error: 0.2133\n",
      "Epoch 228/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1056 - mean_absolute_error: 0.2101 - val_loss: 0.1315 - val_mean_absolute_error: 0.2197\n",
      "Epoch 229/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1101 - mean_absolute_error: 0.2141 - val_loss: 0.1436 - val_mean_absolute_error: 0.2297\n",
      "Epoch 230/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1139 - mean_absolute_error: 0.2156 - val_loss: 0.1097 - val_mean_absolute_error: 0.2086\n",
      "Epoch 231/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1080 - mean_absolute_error: 0.2131 - val_loss: 0.1130 - val_mean_absolute_error: 0.2128\n",
      "Epoch 232/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1076 - mean_absolute_error: 0.2122 - val_loss: 0.1227 - val_mean_absolute_error: 0.2146\n",
      "Epoch 233/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1079 - mean_absolute_error: 0.2142 - val_loss: 0.1157 - val_mean_absolute_error: 0.2028\n",
      "Epoch 234/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1147 - mean_absolute_error: 0.2138 - val_loss: 0.1313 - val_mean_absolute_error: 0.2153\n",
      "Epoch 235/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1119 - mean_absolute_error: 0.2142 - val_loss: 0.1223 - val_mean_absolute_error: 0.2175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1098 - mean_absolute_error: 0.2142 - val_loss: 0.1235 - val_mean_absolute_error: 0.2189\n",
      "Epoch 237/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1099 - mean_absolute_error: 0.2138 - val_loss: 0.1056 - val_mean_absolute_error: 0.2133\n",
      "Epoch 238/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1092 - mean_absolute_error: 0.2133 - val_loss: 0.1038 - val_mean_absolute_error: 0.2058\n",
      "Epoch 239/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1053 - mean_absolute_error: 0.2119 - val_loss: 0.1025 - val_mean_absolute_error: 0.1991\n",
      "Epoch 240/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1150 - mean_absolute_error: 0.2129 - val_loss: 0.1273 - val_mean_absolute_error: 0.2255\n",
      "Epoch 241/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1085 - mean_absolute_error: 0.2124 - val_loss: 0.1157 - val_mean_absolute_error: 0.2029\n",
      "Epoch 242/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1046 - mean_absolute_error: 0.2119 - val_loss: 0.1241 - val_mean_absolute_error: 0.2083\n",
      "Epoch 243/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1107 - mean_absolute_error: 0.2129 - val_loss: 0.1229 - val_mean_absolute_error: 0.2126\n",
      "Epoch 244/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1146 - mean_absolute_error: 0.2149 - val_loss: 0.1132 - val_mean_absolute_error: 0.2084\n",
      "Epoch 245/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1026 - mean_absolute_error: 0.2109 - val_loss: 0.1314 - val_mean_absolute_error: 0.2128\n",
      "Epoch 246/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1059 - mean_absolute_error: 0.2125 - val_loss: 0.1170 - val_mean_absolute_error: 0.2187\n",
      "Epoch 247/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1023 - mean_absolute_error: 0.2103 - val_loss: 0.1052 - val_mean_absolute_error: 0.1991\n",
      "Epoch 248/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1153 - mean_absolute_error: 0.2159 - val_loss: 0.1237 - val_mean_absolute_error: 0.2160\n",
      "Epoch 249/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1111 - mean_absolute_error: 0.2139 - val_loss: 0.1096 - val_mean_absolute_error: 0.2156\n",
      "Epoch 250/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1119 - mean_absolute_error: 0.2111 - val_loss: 0.1150 - val_mean_absolute_error: 0.2112\n",
      "Epoch 251/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1070 - mean_absolute_error: 0.2129 - val_loss: 0.1172 - val_mean_absolute_error: 0.2177\n",
      "Epoch 252/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1036 - mean_absolute_error: 0.2107 - val_loss: 0.1235 - val_mean_absolute_error: 0.2133\n",
      "Epoch 253/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1114 - mean_absolute_error: 0.2139 - val_loss: 0.1247 - val_mean_absolute_error: 0.2279\n",
      "Epoch 254/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1163 - mean_absolute_error: 0.2148 - val_loss: 0.1134 - val_mean_absolute_error: 0.2102\n",
      "Epoch 255/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1073 - mean_absolute_error: 0.2128 - val_loss: 0.1351 - val_mean_absolute_error: 0.2064\n",
      "Epoch 256/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1110 - mean_absolute_error: 0.2137 - val_loss: 0.1330 - val_mean_absolute_error: 0.2095\n",
      "Epoch 257/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1099 - mean_absolute_error: 0.2123 - val_loss: 0.1308 - val_mean_absolute_error: 0.2163\n",
      "Epoch 258/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1025 - mean_absolute_error: 0.2102 - val_loss: 0.1172 - val_mean_absolute_error: 0.2211\n",
      "Epoch 259/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1063 - mean_absolute_error: 0.2119 - val_loss: 0.1241 - val_mean_absolute_error: 0.2261\n",
      "Epoch 260/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1049 - mean_absolute_error: 0.2112 - val_loss: 0.1252 - val_mean_absolute_error: 0.2194\n",
      "Epoch 261/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1105 - mean_absolute_error: 0.2130 - val_loss: 0.1270 - val_mean_absolute_error: 0.2178\n",
      "Epoch 262/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1050 - mean_absolute_error: 0.2100 - val_loss: 0.1099 - val_mean_absolute_error: 0.2043\n",
      "Epoch 263/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1097 - mean_absolute_error: 0.2129 - val_loss: 0.1211 - val_mean_absolute_error: 0.2209\n",
      "Epoch 264/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1120 - mean_absolute_error: 0.2144 - val_loss: 0.1209 - val_mean_absolute_error: 0.2188\n",
      "Epoch 265/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1047 - mean_absolute_error: 0.2125 - val_loss: 0.1242 - val_mean_absolute_error: 0.2176\n",
      "Epoch 266/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1139 - mean_absolute_error: 0.2127 - val_loss: 0.1067 - val_mean_absolute_error: 0.2048\n",
      "Epoch 267/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1086 - mean_absolute_error: 0.2127 - val_loss: 0.1171 - val_mean_absolute_error: 0.2049\n",
      "Epoch 268/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1061 - mean_absolute_error: 0.2123 - val_loss: 0.1128 - val_mean_absolute_error: 0.2178\n",
      "Epoch 269/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1143 - mean_absolute_error: 0.2131 - val_loss: 0.1121 - val_mean_absolute_error: 0.2039\n",
      "Epoch 270/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1154 - mean_absolute_error: 0.2153 - val_loss: 0.1436 - val_mean_absolute_error: 0.2259\n",
      "Epoch 271/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1076 - mean_absolute_error: 0.2136 - val_loss: 0.1202 - val_mean_absolute_error: 0.2122\n",
      "Epoch 272/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1111 - mean_absolute_error: 0.2140 - val_loss: 0.1369 - val_mean_absolute_error: 0.2270\n",
      "Epoch 273/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1064 - mean_absolute_error: 0.2134 - val_loss: 0.1108 - val_mean_absolute_error: 0.1935\n",
      "Epoch 274/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1118 - mean_absolute_error: 0.2132 - val_loss: 0.1359 - val_mean_absolute_error: 0.2203\n",
      "Epoch 275/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1103 - mean_absolute_error: 0.2114 - val_loss: 0.1279 - val_mean_absolute_error: 0.2154\n",
      "Epoch 276/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1063 - mean_absolute_error: 0.2124 - val_loss: 0.1266 - val_mean_absolute_error: 0.2316\n",
      "Epoch 277/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1141 - mean_absolute_error: 0.2142 - val_loss: 0.1062 - val_mean_absolute_error: 0.2080\n",
      "Epoch 278/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1057 - mean_absolute_error: 0.2117 - val_loss: 0.1369 - val_mean_absolute_error: 0.2226\n",
      "Epoch 279/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1106 - mean_absolute_error: 0.2159 - val_loss: 0.1345 - val_mean_absolute_error: 0.2285\n",
      "Epoch 280/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1029 - mean_absolute_error: 0.2104 - val_loss: 0.1116 - val_mean_absolute_error: 0.2092\n",
      "Epoch 281/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1034 - mean_absolute_error: 0.2106 - val_loss: 0.1200 - val_mean_absolute_error: 0.2196\n",
      "Epoch 282/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1072 - mean_absolute_error: 0.2130 - val_loss: 0.1290 - val_mean_absolute_error: 0.2220\n",
      "Epoch 283/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1068 - mean_absolute_error: 0.2109 - val_loss: 0.1209 - val_mean_absolute_error: 0.2132\n",
      "Epoch 284/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1090 - mean_absolute_error: 0.2140 - val_loss: 0.1183 - val_mean_absolute_error: 0.2196\n",
      "Epoch 285/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1152 - mean_absolute_error: 0.2146 - val_loss: 0.1200 - val_mean_absolute_error: 0.2243\n",
      "Epoch 286/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1102 - mean_absolute_error: 0.2125 - val_loss: 0.1286 - val_mean_absolute_error: 0.2134\n",
      "Epoch 287/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1114 - mean_absolute_error: 0.2142 - val_loss: 0.1376 - val_mean_absolute_error: 0.2279\n",
      "Epoch 288/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1055 - mean_absolute_error: 0.2117 - val_loss: 0.1076 - val_mean_absolute_error: 0.2101\n",
      "Epoch 289/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1057 - mean_absolute_error: 0.2109 - val_loss: 0.1218 - val_mean_absolute_error: 0.2203\n",
      "Epoch 290/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1009 - mean_absolute_error: 0.2103 - val_loss: 0.1141 - val_mean_absolute_error: 0.2135\n",
      "Epoch 291/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1083 - mean_absolute_error: 0.2142 - val_loss: 0.1220 - val_mean_absolute_error: 0.2125\n",
      "Epoch 292/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1035 - mean_absolute_error: 0.2119 - val_loss: 0.1357 - val_mean_absolute_error: 0.2099\n",
      "Epoch 293/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1012 - mean_absolute_error: 0.2104 - val_loss: 0.1296 - val_mean_absolute_error: 0.2023\n",
      "Epoch 294/300\n",
      "12967/12967 [==============================] - 0s 36us/sample - loss: 0.1080 - mean_absolute_error: 0.2124 - val_loss: 0.1314 - val_mean_absolute_error: 0.2248\n",
      "Epoch 295/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1161 - mean_absolute_error: 0.2142 - val_loss: 0.1248 - val_mean_absolute_error: 0.2130\n",
      "Epoch 296/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1259 - mean_absolute_error: 0.2161 - val_loss: 0.1145 - val_mean_absolute_error: 0.2014\n",
      "Epoch 297/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1105 - mean_absolute_error: 0.2144 - val_loss: 0.1278 - val_mean_absolute_error: 0.2072\n",
      "Epoch 298/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1059 - mean_absolute_error: 0.2124 - val_loss: 0.1222 - val_mean_absolute_error: 0.2214\n",
      "Epoch 299/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1052 - mean_absolute_error: 0.2151 - val_loss: 0.1257 - val_mean_absolute_error: 0.2185\n",
      "Epoch 300/300\n",
      "12967/12967 [==============================] - 0s 35us/sample - loss: 0.1069 - mean_absolute_error: 0.2125 - val_loss: 0.1113 - val_mean_absolute_error: 0.2089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f45593b6908>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4 = keras.Sequential(name='model-4')\n",
    "model_4.add(layers.Dense(64, activation='relu', input_shape=(21,)))\n",
    "model_4.add(layers.Dropout(0.3))\n",
    "model_4.add(layers.Dense(64, activation='relu'))\n",
    "model_4.add(layers.Dropout(0.3))\n",
    "model_4.add(layers.Dense(1))\n",
    "\n",
    "model_4.compile(keras.optimizers.Adam(0.001),\n",
    "                loss=keras.losses.MeanSquaredError(),\n",
    "                metrics=[keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "log_dir = os.path.join('lab2-logs', 'model-4')\n",
    "model_cbk = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "model_mckp = keras.callbacks.ModelCheckpoint(model_dir + '/Best-model-4.h5', \n",
    "                                             monitor='val_mean_absolute_error', \n",
    "                                             save_best_only=True, \n",
    "                                             mode='min')\n",
    "model_4.fit(x_train, y_train, \n",
    "            batch_size=64 ,\n",
    "            epochs=300, \n",
    "            validation_data=(x_val, y_val), \n",
    "            callbacks=[model_cbk, model_mckp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 驗證正則化的效能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_2: 13.15%\n"
     ]
    }
   ],
   "source": [
    "model_2 = keras.models.load_model('lab2-logs/models/Best-model-2.h5')\n",
    "y_pred = model_2.predict(x_test)\n",
    "y_pred = np.reshape(y_pred * std['price'] + mean['price'], y_test.shape)\n",
    "percentage_error = np.mean(np.abs(y_test - y_pred)) / np.mean(y_test) * 100\n",
    "print(\"Model_2: {:.2f}%\".format(percentage_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_3: 12.89%\n"
     ]
    }
   ],
   "source": [
    "model_3 = keras.models.load_model('lab2-logs/models/Best-model-3.h5')\n",
    "y_pred = model_3.predict(x_test)\n",
    "y_pred = np.reshape(y_pred * std['price'] + mean['price'], y_test.shape)\n",
    "percentage_error = np.mean(np.abs(y_test - y_pred)) / np.mean(y_test) * 100\n",
    "print(\"Model_3: {:.2f}%\".format(percentage_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_4: 13.33%\n"
     ]
    }
   ],
   "source": [
    "model_4 = keras.models.load_model('lab2-logs/models/Best-model-4.h5')\n",
    "y_pred = model_4.predict(x_test)\n",
    "y_pred = np.reshape(y_pred * std['price'] + mean['price'], y_test.shape)\n",
    "percentage_error = np.mean(np.abs(y_test - y_pred)) / np.mean(y_test) * 100\n",
    "print(\"Model_4: {:.2f}%\".format(percentage_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
